# Comparing `tmp/datarobot_mlops-9.0.7-py2.py3-none-any.whl.zip` & `tmp/datarobot_mlops-9.1.1b1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,40 +1,40 @@
-Zip file size: 88933 bytes, number of entries: 38
--rw-r--r--  2.0 unx      597 b- defN 23-Apr-07 22:16 datarobot/mlops/__init__.py
--rw-r--r--  2.0 unx    21600 b- defN 23-Apr-07 22:16 datarobot/mlops/agent.py
--rw-r--r--  2.0 unx     1174 b- defN 23-Apr-07 22:16 datarobot/mlops/constants.py
--rw-r--r--  2.0 unx     3933 b- defN 23-Apr-07 22:16 datarobot/mlops/event.py
--rw-r--r--  2.0 unx      754 b- defN 23-Apr-07 22:16 datarobot/mlops/install_aliases.py
--rw-r--r--  2.0 unx     3301 b- defN 23-Apr-07 22:16 datarobot/mlops/json_shim.py
--rw-r--r--  2.0 unx    31394 b- defN 23-Apr-07 22:16 datarobot/mlops/metric.py
--rw-r--r--  2.0 unx    39699 b- defN 23-Apr-07 22:16 datarobot/mlops/mlops.py
--rw-r--r--  2.0 unx    36876 b- defN 23-Apr-07 22:16 datarobot/mlops/model.py
--rw-r--r--  2.0 unx      532 b- defN 23-Apr-07 22:16 datarobot/mlops/channel/__init__.py
--rw-r--r--  2.0 unx    19456 b- defN 23-Apr-07 22:16 datarobot/mlops/channel/output_channel_queue.py
--rw-r--r--  2.0 unx    13720 b- defN 23-Apr-07 22:16 datarobot/mlops/channel/record.py
--rw-r--r--  2.0 unx      532 b- defN 23-Apr-07 22:16 datarobot/mlops/common/__init__.py
--rw-r--r--  2.0 unx     8379 b- defN 23-Apr-07 22:16 datarobot/mlops/common/aggregation_util.py
--rw-r--r--  2.0 unx    17494 b- defN 23-Apr-07 22:16 datarobot/mlops/common/config.py
--rw-r--r--  2.0 unx     2679 b- defN 23-Apr-07 22:16 datarobot/mlops/common/enums.py
--rw-r--r--  2.0 unx     1285 b- defN 23-Apr-07 22:16 datarobot/mlops/common/exception.py
--rw-r--r--  2.0 unx     2411 b- defN 23-Apr-07 22:16 datarobot/mlops/common/prediction_util.py
--rw-r--r--  2.0 unx     1065 b- defN 23-Apr-07 22:16 datarobot/mlops/common/singleton.py
--rw-r--r--  2.0 unx     1123 b- defN 23-Apr-07 22:16 datarobot/mlops/common/stringutil.py
--rw-r--r--  2.0 unx      532 b- defN 23-Apr-07 22:16 datarobot/mlops/spooler/__init__.py
--rw-r--r--  2.0 unx     7626 b- defN 23-Apr-07 22:16 datarobot/mlops/spooler/async_memory_spooler.py
--rw-r--r--  2.0 unx    32861 b- defN 23-Apr-07 22:16 datarobot/mlops/spooler/filesystem_spooler.py
--rw-r--r--  2.0 unx    22445 b- defN 23-Apr-07 22:16 datarobot/mlops/spooler/kafka_spooler.py
--rw-r--r--  2.0 unx     7363 b- defN 23-Apr-07 22:16 datarobot/mlops/spooler/memory_spooler.py
--rw-r--r--  2.0 unx    12670 b- defN 23-Apr-07 22:16 datarobot/mlops/spooler/pubsub_spooler.py
--rw-r--r--  2.0 unx    10529 b- defN 23-Apr-07 22:16 datarobot/mlops/spooler/rabbitmq_spooler.py
--rw-r--r--  2.0 unx     4681 b- defN 23-Apr-07 22:16 datarobot/mlops/spooler/record_spooler.py
--rw-r--r--  2.0 unx     4685 b- defN 23-Apr-07 22:16 datarobot/mlops/spooler/record_spooler_factory.py
--rw-r--r--  2.0 unx     4749 b- defN 23-Apr-07 22:16 datarobot/mlops/spooler/spooler_offset_manager.py
--rw-r--r--  2.0 unx    10780 b- defN 23-Apr-07 22:16 datarobot/mlops/spooler/sqs_spool.py
--rw-r--r--  2.0 unx     2509 b- defN 23-Apr-07 22:16 datarobot/mlops/spooler/stdout_spooler.py
--rw-r--r--  2.0 unx     2570 b- defN 23-Apr-07 22:21 datarobot_mlops-9.0.7.data/data/share/mlops/agent.yaml
--rw-r--r--  2.0 unx     1395 b- defN 23-Apr-07 22:21 datarobot_mlops-9.0.7.data/data/share/mlops/mlops.log4j2.properties
--rw-r--r--  2.0 unx     3184 b- defN 23-Apr-07 22:21 datarobot_mlops-9.0.7.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-Apr-07 22:21 datarobot_mlops-9.0.7.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 23-Apr-07 22:21 datarobot_mlops-9.0.7.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3544 b- defN 23-Apr-07 22:21 datarobot_mlops-9.0.7.dist-info/RECORD
-38 files, 340247 bytes uncompressed, 83167 bytes compressed:  75.6%
+Zip file size: 88918 bytes, number of entries: 38
+-rw-r--r--  2.0 unx      597 b- defN 23-Mar-27 23:33 datarobot/mlops/__init__.py
+-rw-r--r--  2.0 unx    21276 b- defN 23-Mar-27 23:33 datarobot/mlops/agent.py
+-rw-r--r--  2.0 unx     1023 b- defN 23-Mar-27 23:33 datarobot/mlops/constants.py
+-rw-r--r--  2.0 unx     4241 b- defN 23-Mar-27 23:33 datarobot/mlops/event.py
+-rw-r--r--  2.0 unx     3068 b- defN 23-Mar-27 23:33 datarobot/mlops/json_shim.py
+-rw-r--r--  2.0 unx    30722 b- defN 23-Mar-27 23:33 datarobot/mlops/metric.py
+-rw-r--r--  2.0 unx    39436 b- defN 23-Mar-27 23:33 datarobot/mlops/mlops.py
+-rw-r--r--  2.0 unx    43371 b- defN 23-Mar-27 23:33 datarobot/mlops/model.py
+-rw-r--r--  2.0 unx      532 b- defN 23-Mar-27 23:33 datarobot/mlops/channel/__init__.py
+-rw-r--r--  2.0 unx    19171 b- defN 23-Mar-27 23:33 datarobot/mlops/channel/output_channel_queue.py
+-rw-r--r--  2.0 unx    13375 b- defN 23-Mar-27 23:33 datarobot/mlops/channel/record.py
+-rw-r--r--  2.0 unx      532 b- defN 23-Mar-27 23:33 datarobot/mlops/common/__init__.py
+-rw-r--r--  2.0 unx     8149 b- defN 23-Mar-27 23:33 datarobot/mlops/common/aggregation_util.py
+-rw-r--r--  2.0 unx    16970 b- defN 23-Mar-27 23:33 datarobot/mlops/common/config.py
+-rw-r--r--  2.0 unx     2409 b- defN 23-Mar-27 23:33 datarobot/mlops/common/enums.py
+-rw-r--r--  2.0 unx     1081 b- defN 23-Mar-27 23:33 datarobot/mlops/common/exception.py
+-rw-r--r--  2.0 unx     2182 b- defN 23-Mar-27 23:33 datarobot/mlops/common/prediction_util.py
+-rw-r--r--  2.0 unx      861 b- defN 23-Mar-27 23:33 datarobot/mlops/common/singleton.py
+-rw-r--r--  2.0 unx      919 b- defN 23-Mar-27 23:33 datarobot/mlops/common/stringutil.py
+-rw-r--r--  2.0 unx     3872 b- defN 23-Mar-27 23:33 datarobot/mlops/common/version_util.py
+-rw-r--r--  2.0 unx      532 b- defN 23-Mar-27 23:33 datarobot/mlops/spooler/__init__.py
+-rw-r--r--  2.0 unx     7227 b- defN 23-Mar-27 23:33 datarobot/mlops/spooler/async_memory_spooler.py
+-rw-r--r--  2.0 unx    34711 b- defN 23-Mar-27 23:33 datarobot/mlops/spooler/filesystem_spooler.py
+-rw-r--r--  2.0 unx    21987 b- defN 23-Mar-27 23:33 datarobot/mlops/spooler/kafka_spooler.py
+-rw-r--r--  2.0 unx     6972 b- defN 23-Mar-27 23:33 datarobot/mlops/spooler/memory_spooler.py
+-rw-r--r--  2.0 unx    12434 b- defN 23-Mar-27 23:33 datarobot/mlops/spooler/pubsub_spooler.py
+-rw-r--r--  2.0 unx    10100 b- defN 23-Mar-27 23:33 datarobot/mlops/spooler/rabbitmq_spooler.py
+-rw-r--r--  2.0 unx     4413 b- defN 23-Mar-27 23:33 datarobot/mlops/spooler/record_spooler.py
+-rw-r--r--  2.0 unx     4466 b- defN 23-Mar-27 23:33 datarobot/mlops/spooler/record_spooler_factory.py
+-rw-r--r--  2.0 unx     4454 b- defN 23-Mar-27 23:33 datarobot/mlops/spooler/spooler_offset_manager.py
+-rw-r--r--  2.0 unx    10369 b- defN 23-Mar-27 23:33 datarobot/mlops/spooler/sqs_spool.py
+-rw-r--r--  2.0 unx     2212 b- defN 23-Mar-27 23:33 datarobot/mlops/spooler/stdout_spooler.py
+-rw-r--r--  2.0 unx     2570 b- defN 23-Mar-27 23:38 datarobot_mlops-9.1.1b1.data/data/share/mlops/agent.yaml
+-rw-r--r--  2.0 unx     1395 b- defN 23-Mar-27 23:38 datarobot_mlops-9.1.1b1.data/data/share/mlops/mlops.log4j2.properties
+-rw-r--r--  2.0 unx     3107 b- defN 23-Mar-27 23:38 datarobot_mlops-9.1.1b1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Mar-27 23:38 datarobot_mlops-9.1.1b1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 23-Mar-27 23:38 datarobot_mlops-9.1.1b1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3558 b- defN 23-Mar-27 23:38 datarobot_mlops-9.1.1b1.dist-info/RECORD
+38 files, 344396 bytes uncompressed, 83120 bytes compressed:  75.9%
```

## zipnote {}

```diff
@@ -6,17 +6,14 @@
 
 Filename: datarobot/mlops/constants.py
 Comment: 
 
 Filename: datarobot/mlops/event.py
 Comment: 
 
-Filename: datarobot/mlops/install_aliases.py
-Comment: 
-
 Filename: datarobot/mlops/json_shim.py
 Comment: 
 
 Filename: datarobot/mlops/metric.py
 Comment: 
 
 Filename: datarobot/mlops/mlops.py
@@ -54,14 +51,17 @@
 
 Filename: datarobot/mlops/common/singleton.py
 Comment: 
 
 Filename: datarobot/mlops/common/stringutil.py
 Comment: 
 
+Filename: datarobot/mlops/common/version_util.py
+Comment: 
+
 Filename: datarobot/mlops/spooler/__init__.py
 Comment: 
 
 Filename: datarobot/mlops/spooler/async_memory_spooler.py
 Comment: 
 
 Filename: datarobot/mlops/spooler/filesystem_spooler.py
@@ -90,26 +90,26 @@
 
 Filename: datarobot/mlops/spooler/sqs_spool.py
 Comment: 
 
 Filename: datarobot/mlops/spooler/stdout_spooler.py
 Comment: 
 
-Filename: datarobot_mlops-9.0.7.data/data/share/mlops/agent.yaml
+Filename: datarobot_mlops-9.1.1b1.data/data/share/mlops/agent.yaml
 Comment: 
 
-Filename: datarobot_mlops-9.0.7.data/data/share/mlops/mlops.log4j2.properties
+Filename: datarobot_mlops-9.1.1b1.data/data/share/mlops/mlops.log4j2.properties
 Comment: 
 
-Filename: datarobot_mlops-9.0.7.dist-info/METADATA
+Filename: datarobot_mlops-9.1.1b1.dist-info/METADATA
 Comment: 
 
-Filename: datarobot_mlops-9.0.7.dist-info/WHEEL
+Filename: datarobot_mlops-9.1.1b1.dist-info/WHEEL
 Comment: 
 
-Filename: datarobot_mlops-9.0.7.dist-info/top_level.txt
+Filename: datarobot_mlops-9.1.1b1.dist-info/top_level.txt
 Comment: 
 
-Filename: datarobot_mlops-9.0.7.dist-info/RECORD
+Filename: datarobot_mlops-9.1.1b1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## datarobot/mlops/agent.py

```diff
@@ -1,38 +1,30 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import json
 import logging
 import os
 import re
 import shlex
 import shutil
 import socket
 import subprocess
 import sys
 import tempfile
 import time
-from builtins import int
-from builtins import object
-from builtins import open
-from builtins import str
 
-import datarobot.mlops.install_aliases  # noqa: F401
 import yaml
+from py4j.java_gateway import CallbackServerParameters
+from py4j.java_gateway import GatewayParameters
+from py4j.java_gateway import JavaGateway
+
 from datarobot.mlops.common import config
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.exception import DRApiException
+from datarobot.mlops.common.version_util import DataRobotAppVersion
 from datarobot.mlops.constants import Constants
-from py4j.java_gateway import CallbackServerParameters
-from py4j.java_gateway import GatewayParameters
-from py4j.java_gateway import JavaGateway
 
 #  Copyright (c) 2020 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
@@ -57,15 +49,15 @@
 logger = logging.getLogger(__name__)
 
 AGENT_ENTRY_POINT_CLASS = "com.datarobot.mlops.agent.GatewayEntryPoint"
 MLOPS_URL_DEFAULT = "https://app.datarobot.com/"
 CLEAN_SHUTDOWN_MESSAGE = "Clean Shutdown"
 
 
-class Agent(object):
+class Agent:
     def __init__(
         self, datarobot_url, token, agent_jar_path=None, path_prefix=None, verify_ssl=True
     ):
         if datarobot_url is None:
             raise DRApiException("Missing DataRobot URL")
         if token is None:
             raise DRApiException("Missing User token")
@@ -123,25 +115,25 @@
         self._validate_and_set_spooler()
         self._update_agent_config()
         self._init_process()
 
     def _set_path_prefix(self):
         if self._path_prefix is None:
             self._path_prefix = tempfile.mkdtemp()
-            logger.info("Created temporary directory {}".format(self._path_prefix))
+            logger.info(f"Created temporary directory {self._path_prefix}")
             self._cleanup_path = True
 
     def _validate_and_set_jar_path(self):
         """
         Find the path to the Tracking-Agent JAR file
         :return:
         """
         if not os.path.isfile(self._agent_jar_path):
             raise DRApiException("Invalid agent jar path specified: " + self._agent_jar_path)
-        logger.info("Using monitoring agent at location: {}".format(self._agent_jar_path))
+        logger.info(f"Using monitoring agent at location: {self._agent_jar_path}")
 
     def _set_config_path(self):
         # get Agent config from install in the virtualenv
         self._config_path = os.path.join(self._conf_host_path, AGENT_CONFIG_FILE)
         if not os.path.isfile(self._config_path):
             raise Exception("Error: Agent configuration file not found")
 
@@ -362,26 +354,24 @@
         gateway_running = False
         timeout = min(timeout, 120)
         while timeout > 0:
             time.sleep(1)
             try:
                 int_port = int(port)
                 addr = "127.0.0.1"
-                logger.info("Connecting to java process: addr: {} port: {}".format(addr, int_port))
+                logger.info(f"Connecting to java process: addr: {addr} port: {int_port}")
                 s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                 s.connect((addr, int_port))
                 s.shutdown(socket.SHUT_RDWR)
                 gateway_running = True
                 logger.info("Gateway is up and running")
                 break
             except Exception as e:
                 timeout -= 1
-                logger.info(
-                    "Got error connecting to gateway: timeout {} error: {}".format(timeout, e)
-                )
+                logger.info(f"Got error connecting to gateway: timeout {timeout} error: {e}")
                 continue
 
         if not gateway_running:
             raise DRApiException(
                 "Agent gateway is not running yet on port: {}, giving up\nCmdline: {}".format(
                     port, self._cmdline
                 )
@@ -406,16 +396,16 @@
                 raise ex
 
             if agent_running:
                 logger.info("MLOps agent is up and running")
                 break
 
             if agent_exception:
-                logger.error("Agent got exception: {}".format(agent_exception))
-                raise Exception("Got exception while running agent: {}".format(agent_exception))
+                logger.error(f"Agent got exception: {agent_exception}")
+                raise Exception(f"Got exception while running agent: {agent_exception}")
             timeout -= 1
 
         if not agent_running:
             raise Exception("Failed to start agent\n config: " + json.dumps(self._agent_config))
 
     def _get_agent_version(self):
         """
@@ -429,15 +419,15 @@
     def _ensure_mlops_agent_is_compatible(self):
         """
         The agent is up and running; check whether it is compatible with this version of MLOps.
         For now, we check for identical (major.minor.patch) version strings.
         Raise (or forward) Exception if the version does not match or cannot be retrieved.
         """
         try:
-            agent_version = self._get_agent_version()
+            agent_version = DataRobotAppVersion(string_version=self._get_agent_version())
         except Exception:
             message = (
                 "Failed to determine Agent version. "
                 + "This MLOps library requires Agent version: "
                 + Constants.MLOPS_VERSION
             )
             logger.debug(
@@ -493,21 +483,21 @@
             if isinstance(value, dict):
                 total += sum(value.values())
             else:
                 total += value
         return total
 
     def cleanup(self, message="Execution interrupted"):
-        logger.debug("Cleanup: {}".format(message))
+        logger.debug(f"Cleanup: {message}")
 
         if self._process is not None:
             try:
                 self._process.terminate()
             except Exception as ex:
-                logger.info("Exception occurred: {}".format(ex))
+                logger.info(f"Exception occurred: {ex}")
 
             logger.info("Stopping MLOps Agent")
             if message != CLEAN_SHUTDOWN_MESSAGE:
                 logger.info(
                     "Agent stdout \n {}".format(
                         "".join([line.decode("utf-8") for line in self._process.stdout.readlines()])
                     )
@@ -517,15 +507,15 @@
                         "".join([line.decode("utf-8") for line in self._process.stderr.readlines()])
                     )
                 )
 
             self._process = None
             self._process_args = None
         if self._cleanup_path:
-            logger.info("Removing temporary directory: {}".format(self._path_prefix))
+            logger.info(f"Removing temporary directory: {self._path_prefix}")
             # In case directory is not empty or other error
             # 'ignore_errors=True' will not raise exception
             shutil.rmtree(self._path_prefix, ignore_errors=True)
 
     def wait_for_stats_sent_to_mmm(self, submitted_stats, timeout=3600):
         if len(submitted_stats) == 0:
             return
```

## datarobot/mlops/constants.py

```diff
@@ -1,12 +1,7 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 #  Copyright (c) 2019 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
@@ -14,21 +9,21 @@
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
 
 # WARNING: This file must never import any 3rd party modules because it is used in setup.py.
 
 
-class Constants(object):
+class Constants:
     """
     Various constants related to MLOps
     """
 
     OFFICIAL_NAME = "datarobot-mlops"
-    MLOPS_VERSION = "9.0.7"
+    MLOPS_VERSION = "9.1.1-beta.1"
 
     # Constants used to upload actuals
     ACTUALS_ASSOCIATION_ID_KEY = "associationId"
     ACTUALS_WAS_ACTED_ON_KEY = "wasActedOn"
     ACTUALS_TIMESTAMP_KEY = "timestamp"
     ACTUALS_VALUE_KEY = "actualValue"
```

## datarobot/mlops/event.py

```diff
@@ -1,34 +1,30 @@
 """
 Model external (aka "remote") MLOps events that are sent to DataRobot via API.
 """
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
 
-from builtins import object
 from datetime import datetime
 from enum import Enum
 
-import datarobot.mlops.install_aliases  # noqa: F401
-
 #  Copyright (c) 2020 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
 #
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
 
 
+EVENT_MESSAGE_MAX_LENGTH = 16384
+
+
 class EventType(Enum):
     """
     Helper symbols to avoid having to memorize event type strings.
     """
 
     PRED_REQUEST_FAILED = "prediction_request.failed"
     DEP_ACCURACY_GREEN = "model_deployments.accuracy_green"
@@ -37,47 +33,48 @@
     DEP_DRIFT_GREEN = "model_deployments.data_drift_green"
     DEP_DRIFT_RED = "model_deployments.data_drift_red"
     DEP_DRIFT_YELLOW = "model_deployments.data_drift_yellow_from_green"
     DEP_MODEL_REPLACED = "model_deployments.model_replacement"
     DEP_SERVICE_GREEN = "model_deployments.service_health_green"
     DEP_SERVICE_RED = "model_deployments.service_health_red"
     DEP_SERVICE_YELLOW = "model_deployments.service_health_yellow_from_green"
+    EXTERNAL_NAN_PREDICTIONS = "externalNaNPredictions"
 
 
-class Event(object):
+class Event:
     """
     Represents an external event.
     The MLOps client creates this object.
     """
 
     def __init__(
         self,
         event_type,
         message,
         entity_id=None,
         org_id=None,
         data=None,
-        prediction_request_data=None,
     ):
-        # type: (EventType, str, str, str, dict, dict) -> None
+        # type: (EventType, str, str, str, dict) -> None
         """
         :param event_type: event type; use EventType enum
         :param message: string message to accompany event
         :param entity_id: ID of deployment or other entity involved in event. Can be auto-filled.
         :param org_id: ID of organization (if orgs are in use)
-        :param data: optional data struct, depends on event type
-        :param prediction_request_data: optional data struct,
-               contains information about prediction request
+        :param data: data struct, more information about the event, depends on event type
         """
         self._event_type = event_type.value
-        self._message = message
+        if message:
+            if len(message) < EVENT_MESSAGE_MAX_LENGTH:
+                self._message = message
+            else:
+                self._message = message[: (EVENT_MESSAGE_MAX_LENGTH - 1)]
         self._entity_id = entity_id  # deployment, etc.
         self._org_id = org_id
         self._data = data
-        self._prediction_request_data = prediction_request_data
         self._timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f%z")
 
     def get_event_type(self):
         return self._event_type
 
     def get_message(self):
         return self._message
@@ -87,17 +84,14 @@
 
     def get_org_id(self):
         return self._org_id
 
     def get_data(self):
         return self._data
 
-    def get_prediction_request_data(self):
-        return self._prediction_request_data
-
     def get_timestamp(self):
         return self._timestamp
 
     @staticmethod
     def is_entity_a_deployment():
         """
         True if the event's entityId should automatically be set to the
@@ -110,7 +104,22 @@
     def set_entity_id(self, entity_id):
         """
         The MLOps instance knows its deployment ID;
         when entity_id should be deployment ID, it can set this automatically.
         :param entity_id: ObjectID for deployment or other entity
         """
         self._entity_id = entity_id
+
+
+class ExternalNaNPredictionsEvent(Event):
+    def __init__(self, deployment_id, model_id, nan_prediction_indices):
+        # Generate ExternalNaNPredictions event
+        message = "External NaN Predictions for indices: {}".format(
+            ", ".join([str(index) for index in nan_prediction_indices])
+        )
+
+        super(ExternalNaNPredictionsEvent, self).__init__(
+            EventType.EXTERNAL_NAN_PREDICTIONS,
+            message,
+            entity_id=deployment_id,
+            data={"modelId": str(model_id), "count": len(nan_prediction_indices)},
+        )
```

## datarobot/mlops/json_shim.py

```diff
@@ -5,35 +5,28 @@
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
 #
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
 
 import os
 from decimal import Decimal
 
-import datarobot.mlops.install_aliases  # noqa: F401
-import six
-
 
 # For loading JSON, both libraries have similar enough APIs that we can use the same function
 def json_loads(data):
     return loads(data)
 
 
 # Orjson is a Python 3 only library that has been measured to be much faster than all other
 # JSON libraries. However, it is also **not** a drop-in replacement for the standard library
 # so a shim is required until we can drop Python 2 support.
-if six.PY2 or os.environ.get("MLOPS_DISABLE_FAST_JSON") == "1":
+if os.environ.get("MLOPS_DISABLE_FAST_JSON") == "1":
     from json import dumps
     from json import loads
 
     import numpy
 
     def json_dumps_str(obj, default=None):
         return dumps(obj, default=default)
@@ -42,23 +35,23 @@
     def json_dumps_bytes(obj, default=None):
         return dumps(obj, default=default).encode("utf-8")
 
     # Using a `default` argument can impact performance so we leave it on the responsibility of the
     # caller to pass this function in when they think it is necessary
     def default_serializer(obj):
         """
-        Help serialize a few extra datatypes we commonly see, especially in Prediction Data payloads
+        Help serialize a few extra datatypes we commonly see, especially in Prediction Data payloads.
         """
         if isinstance(obj, Decimal):
             return float(obj)  # close enough approximation for our needs
         if isinstance(obj, numpy.generic):
             return obj.item()
         if isinstance(obj, numpy.ndarray):
             return obj.tolist()
-        raise TypeError("Type {} not JSON serializable: {}".format(type(obj), obj))
+        raise TypeError(f"Type {type(obj)} not JSON serializable: {obj}")
 
 else:
     from orjson import OPT_SERIALIZE_NUMPY
     from orjson import dumps
     from orjson import loads
 
     def json_dumps_bytes(obj, default=None):
@@ -69,12 +62,12 @@
         return dumps(obj, default=default, option=OPT_SERIALIZE_NUMPY).decode("utf-8")
 
     # Using a `default` argument can impact performance so we leave it on the responsibility of the
     # caller to pass this function in when they think it is necessary. Since orjson has built in
     # handling of NumPy types we only need to deal with Decimal types.
     def default_serializer(obj):
         """
-        Help serialize a few extra datatypes we commonly see, especially in Prediction Data payloads
+        Help serialize a few extra datatypes we commonly see, especially in Prediction Data payloads.
         """
         if isinstance(obj, Decimal):
             return float(obj)  # close enough approximation for our needs
         raise TypeError  # orjson doesn't use exception message so don't bother
```

## datarobot/mlops/metric.py

```diff
@@ -1,43 +1,34 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
-import abc
 import json
+import sys
+from abc import ABC
+from abc import abstractmethod
 
 #  Copyright (c) 2019 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
 #
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
-from builtins import dict
-from builtins import object
-from builtins import range
 from datetime import datetime
 from decimal import Decimal
 from math import isnan
 
-import datarobot.mlops.install_aliases  # noqa: F401
 import pandas as pd
+from dateutil.tz import tzlocal
+
 from datarobot.mlops.common.enums import DataFormat
 from datarobot.mlops.common.enums import DataType
 from datarobot.mlops.common.exception import DRUnsupportedType
 from datarobot.mlops.event import Event
-from dateutil.tz import sys
-from dateutil.tz import tzlocal
-from future.utils import with_metaclass
-from six import string_types
 
 """
 Currently there are the following independent types of information:
 
 GeneralStats: model ID, timestamp
 DeploymentStats: number of predictions, execution time
 PredictionsStats: predictions, class names
@@ -61,65 +52,65 @@
     estimate_size = sys.getsizeof(metric)
 
     if hasattr(metric, "__dict__"):
         estimate_size += sum([sys.getsizeof(x) for x in metric.__dict__.values() if x is not None])
     return estimate_size
 
 
-class SerializationConstants(object):
-    class GeneralConstants(object):
+class SerializationConstants:
+    class GeneralConstants:
         MODEL_ID_FIELD_NAME = "modelId"
         TIMESTAMP_FIELD_NAME = "timestamp"
         BATCH_NAME_FIELD_NAME = "batchName"
 
-    class DeploymentStatsConstants(object):
+    class DeploymentStatsConstants:
         NUM_PREDICTIONS_FIELD_NAME = "numPredictions"
         EXECUTION_TIME_FIELD_NAME = "executionTime"
 
-    class CustomMetricStatsConstants(object):
+    class CustomMetricStatsConstants:
         BUCKETS_FIELD_NAME = "buckets"
         METRIC_ID_FIELD_NAME = "customMetricId"
         MODEL_PACKAGE_ID_FIELD_NAME = "modelPackageId"
         METRIC_VALUE_FIELD_NAME = "value"
 
-    class PredictionsStatsConstants(object):
+    class PredictionsStatsConstants:
         PREDICTIONS_FIELD_NAME = "predictions"
         ASSOCIATION_IDS_FIELD_NAME = "associationIds"
         RESULTS_FIELD_NAME = "results"
         CLASS_NAMES_FIELD_NAME = "classNames"
 
-    class PredictionsDataConstants(object):
+    class PredictionsDataConstants:
         PREDICTIONS_FIELD_NAME = "predictions"
         ASSOCIATION_IDS_FIELD_NAME = "associationIds"
         FEATURES_FIELD_NAME = "features"
         CLASS_NAMES_FIELD_NAME = "classNames"
         REQUEST_PARAMETERS_FIELD_NAME = "requestParameters"
         FORECAST_DISTANCE_FIELD_NAME = "forecastDistance"
         ROW_INDEX_FIELD_NAME = "rowIndex"
         PARTITION_FIELD_NAME = "partition"
         SERIES_ID_FIELD_NAME = "seriesId"
         SKIP_DRIFT_TRACKING_FIELD_NAME = "skipDriftTracking"
         SKIP_ACCURACY_TRACKING_FIELD_NAME = "skipAccuracyTracking"
 
-    class AggregatedStatsConstants(object):
+    class AggregatedStatsConstants:
         NUMERIC_AGGREGATE_MAP = "numericAggregateMap"
         CATEGORICAL_AGGREGATE_MAP = "categoricalAggregateMap"
         PREDICTION_AGGREGATE_MAP = "predictionAggregateMap"
         SEGMENT_ATTRIBUTES_AGGREGATE_STATS = "segmentAttributesAggregatedStats"
         SEGMENT_ATTRIBUTES_FIELD_NAME = "segments"
 
-    class EventConstants(object):
+    class EventConstants:
         EVENT_TYPE_FIELD_NAME = "eventType"
         ORG_ID_FIELD_NAME = "orgId"
         ENTITY_ID_FIELD_NAME = "deploymentId"  # todo this is changing
         MESSAGE_FIELD_NAME = "message"
-        PREDICTION_REQUEST_DATA_FIELD_NAME = "predictionRequestData"
+        DATA_FIELD_NAME = "data"
 
 
-class GeneralStats(object):
+class GeneralStats:
     """
     General statistics.
     """
 
     def __init__(self, model_id, timestamp=None, batch_name=None):
         self._model_id = model_id
         self._batch_name = batch_name
@@ -142,15 +133,15 @@
     def get_timestamp(self):
         return self._timestamp
 
     def get_batch_name(self):
         return self._batch_name
 
 
-class DeploymentStats(object):
+class DeploymentStats:
     """
     Class to keep data about deployment statistics.
     """
 
     def __init__(self, num_predictions, execution_time):
         self._num_predictions = num_predictions
         self._execution_time = execution_time
@@ -158,15 +149,15 @@
     def get_num_predictions(self):
         return self._num_predictions
 
     def get_execution_time(self):
         return self._execution_time
 
 
-class CustomMetric(object):
+class CustomMetric:
     """
     Class to keep data about a custom metric.
     """
 
     def __init__(self, metric_id, values, timestamps):
         self._metric_id = metric_id
         self._values = values
@@ -181,15 +172,15 @@
     def get_values(self):
         return self._values
 
     def get_timestamps(self):
         return self._timestamps
 
 
-class PredictionsStats(object):
+class PredictionsStats:
     """
     Class to keep data about predictions statistics.
     """
 
     def __init__(self, predictions, class_names, association_ids=None):
         self._predictions = predictions
         self._class_names = class_names
@@ -201,27 +192,27 @@
     def get_class_names(self):
         return self._class_names
 
     def get_association_ids(self):
         return self._association_ids
 
 
-class FeatureDataStats(object):
+class FeatureDataStats:
     """
     Class to keep feature data.
     """
 
     def __init__(self, feature_data):
         self._feature_data = feature_data
 
     def get_feature_data(self):
         return self._feature_data
 
 
-class PredictionsData(object):
+class PredictionsData:
     """
     Class to keep features and predictions data together
     """
 
     def __init__(
         self,
         feature_data=None,
@@ -288,15 +279,15 @@
     def skip_drift_tracking(self):
         return self._skip_drift_tracking
 
     def skip_accuracy_tracking(self):
         return self._skip_accuracy_tracking
 
 
-class AggregatedStats(object):
+class AggregatedStats:
     def __init__(
         self,
         numeric_aggregate_map=None,
         categorical_aggregate_map=None,
         prediction_aggregate_map=None,
         segment_attributes_aggregated_stats=None,
         class_names=None,
@@ -319,30 +310,27 @@
     def get_segment_attributes_aggregated_stats(self):
         return self.segment_attributes_aggregated_stats
 
     def get_class_names(self):
         return self._class_names
 
 
-class StatsContainer(with_metaclass(abc.ABCMeta)):
-    def __init(self):
-        pass
-
-    @abc.abstractmethod
+class StatsContainer(ABC):
+    @abstractmethod
     def to_iterable(self):
-        """ """
+        pass
 
-    @abc.abstractmethod
+    @abstractmethod
     def get_estimate_size(self):
         """
         Return current stats estimated size in memory
         :return: estimated size of object in bytes
         """
 
-    @abc.abstractmethod
+    @abstractmethod
     def data_type(self):
         """
         Get type of the data current metric represents.
         Check @DataType.
 
         :return: type of the data for current metric.
         :rtype: DataType
@@ -352,26 +340,26 @@
         if data_format == DataFormat.JSON:
             return self.to_iterable()
         elif data_format == DataFormat.BYTE_ARRAY:
             json_str = json.dumps(self.to_iterable())
             return bytearray(json_str, encoding="utf8")
         else:
             raise NotImplementedError(
-                "Metric serialization does not support data format {}".format(data_format)
+                f"Metric serialization does not support data format {data_format}"
             )
 
     def serialize_iterable(self, data_format, stat_iterable):
         if data_format == DataFormat.JSON:
             return stat_iterable
         elif data_format == DataFormat.BYTE_ARRAY:
             json_str = json.dumps(stat_iterable)
             return bytearray(json_str, encoding="utf8")
         else:
             raise NotImplementedError(
-                "Metric serialization does not support data format {}".format(data_format)
+                f"Metric serialization does not support data format {data_format}"
             )
 
 
 class DeploymentStatsContainer(StatsContainer):
     """
     Deployment stats data formatter.
     """
@@ -735,29 +723,29 @@
                         missing_values += 1
                         continue
                     supported_type = True
                     break
                 if isinstance(val, int):
                     supported_type = True
                     break
-                if isinstance(val, string_types):
+                if isinstance(val, str):
                     supported_type = True
                     break
                 if isinstance(val, Decimal):
                     # Decimal type cannot be json serialized; convert column to float.
                     # TODO: another approach is to convert with a `default` serializer with the
                     # call to `dumps`
                     for k in range(j, len(vals)):
                         vals[k] = float(vals[k])
                     supported_type = True
                     break
                 else:
                     # TODO: once we have a logging mechanism, this should be logged and col skipped
                     raise DRUnsupportedType(
-                        "feature_data field type is not supported '{}'".format(type(vals[j]))
+                        f"feature_data field type is not supported '{type(vals[j])}'"
                     )
 
             # If all values are NaN, then also include the feature
             if missing_values == len(vals):
                 supported_type = True
 
             if supported_type:
@@ -840,37 +828,35 @@
     """
     External event data formatter.
     """
 
     def __init__(self, event):
         if not isinstance(event, Event):
             raise DRUnsupportedType(
-                "Wrong value type for event. Expected: {}, provided: {}".format(Event, type(event))
+                f"Wrong value type for event. Expected: {Event}, provided: {type(event)}"
             )
         self._event = event
         self._estimate_size = None
 
     def get_estimate_size(self):
         if self._estimate_size is None:
             self._estimate_size = estimate_metric_size(self._event)
         return self._estimate_size
 
     def data_type(self):
-        return DataType.EXTERNAL_EVENT
+        return DataType.EVENT
 
     def to_iterable(self):
         ret = dict()
         ret[
             SerializationConstants.GeneralConstants.TIMESTAMP_FIELD_NAME
         ] = self._event.get_timestamp()
         ret[
             SerializationConstants.EventConstants.EVENT_TYPE_FIELD_NAME
         ] = self._event.get_event_type()
         ret[SerializationConstants.EventConstants.MESSAGE_FIELD_NAME] = self._event.get_message()
         ret[SerializationConstants.EventConstants.ORG_ID_FIELD_NAME] = self._event.get_org_id()
         ret[
             SerializationConstants.EventConstants.ENTITY_ID_FIELD_NAME
         ] = self._event.get_entity_id()
-        ret[
-            SerializationConstants.EventConstants.PREDICTION_REQUEST_DATA_FIELD_NAME
-        ] = self._event.get_prediction_request_data()
+        ret[SerializationConstants.EventConstants.DATA_FIELD_NAME] = self._event.get_data()
         return ret
```

## datarobot/mlops/mlops.py

```diff
@@ -1,16 +1,12 @@
 """
 mlops library for reporting statistics.
 
 MLOps library can be used to report ML metrics to DataRobot MLOps for centralized monitoring.
 """
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
 
 import atexit
 import json
 import os
 
 #  Copyright (c) 2019 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
@@ -19,19 +15,16 @@
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
 #
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
-from builtins import object
-from builtins import str
 from enum import Enum
 
-import datarobot.mlops.install_aliases  # noqa: F401
 from datarobot.mlops.agent import Agent
 from datarobot.mlops.common import config
 from datarobot.mlops.common.aggregation_util import convert_dict_to_feature_types
 from datarobot.mlops.common.aggregation_util import validate_feature_types
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.exception import DRAlreadyInitialized
@@ -41,15 +34,15 @@
 
 
 class MLOpsLibMode(Enum):
     EMBEDED_AGENT_MODE = 1
     DAEMON_AGENT_MODE = 2
 
 
-class MLOps(object):
+class MLOps:
 
     # ------------------------------------------------------
     #  INTERNAL FUNCTIONS
     # ------------------------------------------------------
 
     def __init__(self):
         self._initialized = False
```

## datarobot/mlops/model.py

```diff
@@ -1,54 +1,49 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 #  Copyright (c) 2019 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
 #
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
+import copy
 import json
-from builtins import str
+import math
 from datetime import datetime
 
-import datarobot.mlops.install_aliases  # noqa: F401
 import pandas as pd
-import six
+from dateutil.tz import tzlocal
+
 from datarobot.mlops.channel.output_channel_queue import OutputChannelQueueAsync
 from datarobot.mlops.channel.output_channel_queue import OutputChannelQueueSync
 from datarobot.mlops.common import config
 from datarobot.mlops.common.aggregation_util import build_aggregated_stats
 from datarobot.mlops.common.aggregation_util import convert_dict_to_feature_types
 from datarobot.mlops.common.aggregation_util import validate_feature_types
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.exception import DRApiException
 from datarobot.mlops.common.exception import DRCommonException
 from datarobot.mlops.common.exception import DRUnsupportedType
+from datarobot.mlops.event import ExternalNaNPredictionsEvent
 from datarobot.mlops.metric import AggregatedStatsContainer
 from datarobot.mlops.metric import CustomMetric
 from datarobot.mlops.metric import CustomMetricContainer
 from datarobot.mlops.metric import DeploymentStats
 from datarobot.mlops.metric import DeploymentStatsContainer
 from datarobot.mlops.metric import EventContainer
 from datarobot.mlops.metric import GeneralStats
 from datarobot.mlops.metric import PredictionsData
 from datarobot.mlops.metric import PredictionsDataContainer
-from dateutil.tz import tzlocal
-from six import string_types
 
 
-class Model(object):
+class Model:
     DEFAULT_ASYNC_REPORTING = False
     MAX_TS_PREDICTIONS = 10000
     MAX_TS_FEATURE_ROWS = 10000
 
     REQUEST_PARAMETERS_MAPPING = {
         "forecast_point": "forecastPoint",
         "predictions_start_date": "predictionsStartDate",
@@ -112,15 +107,15 @@
         )
 
     def shutdown(self, timeout_sec=0):
         self._report_queue.shutdown(timeout_sec=timeout_sec, final_shutdown=False)
         self._report_queue = None
 
     def _validate_input_association_ids(self, predictions, association_ids):
-        self._validate_parameter(predictions, association_ids, "association ids", string_types)
+        self._validate_parameter(predictions, association_ids, "association ids", str)
         if len(set(association_ids)) != len(association_ids):
             raise DRCommonException(
                 "All association ids should be unique, "
                 "association ids uniquely identify each individual prediction"
             )
 
     def _validate_input_features_and_predictions(self, feature_data_df, predictions):
@@ -130,28 +125,35 @@
                     """The number of feature values for feature '{}' ({}) does not match the number
                       of prediction values {}""".format(
                         feature_name, len(feature_values), len(predictions)
                     )
                 )
 
     def _validate_predictions(self, predictions, class_names):
+        """
+        Returns the list of indexes in the list with invalid prediction values "NaN"
+
+        :param predictions:
+        :param class_names:
+        :return:
+        """
         if not isinstance(predictions, list):
             raise DRUnsupportedType("'predictions' should be a list of probabilities or numbers")
 
         likely_classification_predictions = False
         likely_regression_predictions = False
         class_names_present = False
         likely_num_classes = 0
         if class_names is not None:
             if not isinstance(class_names, list):
                 raise DRUnsupportedType("'class_names' should be a list")
             if len(class_names) < 2:
                 raise DRCommonException("'class_names' should contain at least 2 values")
             for class_name in class_names:
-                if not isinstance(class_name, string_types):
+                if not isinstance(class_name, str):
                     raise DRUnsupportedType(
                         "Each class name is expected to be a string, but received {}".format(
                             type(class_name)
                         )
                     )
             class_names_present = True
             likely_num_classes = len(class_names)
@@ -160,31 +162,43 @@
         if isinstance(first_prediction, list):
             likely_classification_predictions = True
             likely_num_classes = len(first_prediction)
         elif isinstance(first_prediction, float) or isinstance(first_prediction, int):
             likely_regression_predictions = True
         else:
             raise DRUnsupportedType(
-                "Predictions with type '{}' not supported".format(str(type(first_prediction)))
+                f"Predictions with type '{str(type(first_prediction))}' not supported"
             )
 
+        nan_prediction_indexes = []
         # Now verify that the remaining list of elements have the same instance / format
         for index, prediction in enumerate(predictions):
             if (
                 likely_regression_predictions
                 and not isinstance(prediction, float)
                 and not isinstance(prediction, int)
             ):
                 raise DRUnsupportedType(
                     """Invalid prediction '{}' at index '{}', expecting a prediction value of
                     type int or float""".format(
                         str(prediction), index
                     )
                 )
-            if likely_classification_predictions:
+            if likely_regression_predictions:
+                if not (isinstance(prediction, float) or isinstance(prediction, int)):
+                    raise DRUnsupportedType(
+                        """Invalid prediction '{}' at index '{}', expecting a prediction value of
+                        type int or float""".format(
+                            str(prediction), index
+                        )
+                    )
+                if math.isnan(prediction):
+                    nan_prediction_indexes.append(index)
+
+            elif likely_classification_predictions:
                 if not isinstance(prediction, list):
                     raise DRUnsupportedType(
                         """Invalid prediction '{}' at index '{}', expecting list of prediction
                         probabilities""".format(
                             str(prediction), index
                         )
                     )
@@ -221,14 +235,19 @@
                     if prob > 1.0 or prob < 0.0:
                         raise DRCommonException(
                             """Probability value '{}' in prediction '{}' at index '{}' is not
                             between 0 and 1""".format(
                                 prob, prediction, index
                             )
                         )
+                    if math.isnan(prob):
+                        nan_prediction_indexes.append(index)
+                        break
+
+        return nan_prediction_indexes
 
     def _report_stats(self, deployment_id, model_id, stats_serializer):
         """
         This function is used for reporting metrics and events.
         """
         data_type = stats_serializer.data_type()
 
@@ -277,18 +296,18 @@
         :param model_id: Model id to report metric for. If None, metric is a deployment metric.
         :param metric_id: Metric id to use
         :param value: Numeric value to report
         :param timestamp: Timestamp to report for the metric
         """
 
         # If value is a single value pack it with timestamp into a list of items
-        if not isinstance(metric_id, six.string_types):
+        if not isinstance(metric_id, str):
             raise DRUnsupportedType(
                 "Metric id must be of type str - got type ({}) {}".format(
-                    type(metric_id), isinstance(metric_id, six.string_types)
+                    type(metric_id), isinstance(metric_id, str)
                 )
             )
 
         if not isinstance(value, (list, int, float)):
             raise DRUnsupportedType(
                 "Value for custom metric must be either int or float (or list of int or float)"
             )
@@ -374,53 +393,65 @@
         :type skip_drift_tracking: bool
         :param skip_accuracy_tracking: Should the DataRobot App skip accuracy calculation for
             these predictions
         :type skip_accuracy_tracking: bool
         :param batch_name: Name of the batch these statistics belong to
         :type batch_name: str
         """
-        feature_data_df = self._validate_and_copy_feature_predictions(
+        nan_prediction_indexes, reporting_data = self._validate_and_copy_feature_predictions(
             features_df, predictions, class_names, association_ids
         )
+
+        if nan_prediction_indexes:
+            reporting_data = self._remove_nans(nan_prediction_indexes, reporting_data)
+
+            self._report_external_nan_predictions_event(
+                deployment_id, model_id, nan_prediction_indexes
+            )
+
         self._report_metric(
             deployment_id,
             model_id,
-            feature_data_df,
-            predictions,
-            association_ids,
-            class_names,
+            reporting_data["_features_df"],
+            reporting_data["_predictions"],
+            reporting_data["_association_ids"],
+            reporting_data["_class_names"],
             skip_drift_tracking=skip_drift_tracking,
             skip_accuracy_tracking=skip_accuracy_tracking,
             batch_name=batch_name,
         )
 
     def _validate_and_copy_feature_predictions(
         self,
         features_df=None,
         predictions=None,
         class_names=None,
         association_ids=None,
     ):
-        if features_df is None and not predictions:
+        if features_df is None and predictions is None:
             raise DRCommonException("One of `features_df` or `predictions` argument is required")
-        if predictions:
-            self._validate_predictions(predictions, class_names)
+        nan_prediction_indexes = None
+        if predictions is not None:
+            nan_prediction_indexes = self._validate_predictions(predictions, class_names)
         if features_df is not None and not isinstance(features_df, pd.DataFrame):
-            raise DRUnsupportedType(
-                "features_df argument has to be of type '{}'".format(pd.DataFrame)
-            )
+            raise DRUnsupportedType(f"features_df argument has to be of type '{pd.DataFrame}'")
         if predictions and association_ids:
             self._validate_input_association_ids(predictions, association_ids)
         # If dataframe provided we do a deep copy, in case is modified before processing
-        feature_data_df = None
-        if features_df is not None:
-            feature_data_df = features_df.copy(deep=True)
-        if feature_data_df is not None and predictions:
-            self._validate_input_features_and_predictions(feature_data_df, predictions)
-        return feature_data_df
+        if features_df is not None and predictions:
+            self._validate_input_features_and_predictions(features_df, predictions)
+        # Deep copy the values
+        reporting_data = self._deep_copy_reporting_data(
+            features_df=features_df,
+            predictions=predictions,
+            association_ids=association_ids,
+            class_names=class_names,
+        )
+
+        return nan_prediction_indexes, reporting_data
 
     def report_aggregated_predictions_data(
         self,
         deployment_id,
         model_id,
         features_df=None,
         predictions=None,
@@ -455,36 +486,50 @@
         :type class_names: list
         :param batch_name: Name of the batch these statistics belong to
         :type batch_name: str
         """
         if features_df is not None and self._feature_types is None:
             raise DRCommonException("Features type should be provided during MLOPS initialization")
 
-        feature_data_df = self._validate_and_copy_feature_predictions(
+        (nan_prediction_indexes, reporting_data) = self._validate_and_copy_feature_predictions(
             features_df, predictions, class_names
         )
-        predictions_df = self._convert_predictions_to_df(predictions, class_names)
+        if nan_prediction_indexes:
+            reporting_data = self._remove_nans(nan_prediction_indexes, reporting_data)
+
+            self._report_external_nan_predictions_event(
+                deployment_id, model_id, nan_prediction_indexes
+            )
+
+        predictions_df = self._convert_predictions_to_df(
+            reporting_data["_predictions"], reporting_data["_class_names"]
+        )
 
         # If the prediction timestamp column is not set or is not present in the feature list
         # follow the regular path
         if (
             self._prediction_timestamp_column_name
             and self._prediction_timestamp_column_format
-            and self._prediction_timestamp_column_name in feature_data_df.columns
+            and self._prediction_timestamp_column_name in reporting_data["_features_df"].columns
         ):
             # Split rows based on timestamps
             self._process_data_for_aggregation_with_prediction_timestamp(
-                deployment_id, model_id, feature_data_df, predictions_df, class_names, batch_name
+                deployment_id,
+                model_id,
+                reporting_data["_features_df"],
+                predictions_df,
+                reporting_data["_class_names"],
+                batch_name,
             )
         else:
             # Call method to report aggregated stats directly
             self._aggregate_stats(
                 deployment_id,
                 model_id,
-                feature_data_df=features_df,
+                feature_data_df=reporting_data["_features_df"],
                 predictions_df=predictions_df,
                 class_names=class_names,
                 batch_name=batch_name,
             )
 
     def _process_data_for_aggregation_with_prediction_timestamp(
         self,
@@ -643,66 +688,165 @@
         :param skip_accuracy_tracking: Should the DataRobot App skip accuracy calculation for
             these predictions
         :type skip_accuracy_tracking: bool
         """
         if features_df is None and not predictions:
             raise DRCommonException("One of `features_df` or `predictions` argument is required")
 
+        nan_prediction_indexes = None
         if predictions:
-            self._validate_predictions(predictions, class_names)
-            if len(predictions) > self.MAX_TS_PREDICTIONS:
+            nan_prediction_indexes = self._validate_predictions(predictions, class_names)
+            if len(predictions) - len(nan_prediction_indexes) > self.MAX_TS_PREDICTIONS:
                 raise DRCommonException(
                     """MLOps library currently supports posting only {} predictions in
                      a single call""".format(
                         self.MAX_TS_PREDICTIONS
                     )
                 )
             # Validate time series prediction report
             self._validate_time_series_prediction_report(
-                predictions, forecast_distance, row_index, partition, series_id
+                predictions, forecast_distance, row_index, partition
             )
             series_id = self._validate_series_id(predictions, series_id)
             if association_ids:
                 self._validate_input_association_ids(predictions, association_ids)
 
         if features_df is not None and not isinstance(features_df, pd.DataFrame):
             raise DRUnsupportedType("features_df argument has to be of type '{}'", pd.DataFrame)
 
-        # If dataframe provided we do a deep copy, in case is modified before processing
-        feature_data_df = None
         if features_df is not None:
             if features_df.shape[0] > self.MAX_TS_FEATURE_ROWS:
                 raise DRCommonException(
                     """MLOps library currently supports posting only {} feature rows in
                      a single call""".format(
                         self.MAX_TS_FEATURE_ROWS
                     )
                 )
-            feature_data_df = features_df.copy(deep=True)
 
-        # Validate and modify request parameters
-        if request_parameters:
-            request_parameters = self._update_request_parameters(request_parameters)
-
-        self._report_metric(
-            deployment_id,
-            model_id,
-            feature_data=feature_data_df,
+        # Deep copy the values
+        reporting_data = self._deep_copy_reporting_data(
+            features_df=features_df,
             predictions=predictions,
             association_ids=association_ids,
-            class_names=class_names,
-            request_parameters=request_parameters,
             forecast_distance=forecast_distance,
             row_index=row_index,
             partition=partition,
             series_id=series_id,
+            class_names=class_names,
+            request_parameters=request_parameters,
+        )
+
+        if nan_prediction_indexes:
+            # Remove NaN predictions and corresponding rows
+            reporting_data = self._remove_nans(nan_prediction_indexes, reporting_data)
+
+            self._report_external_nan_predictions_event(
+                deployment_id, model_id, nan_prediction_indexes
+            )
+
+        self._report_metric(
+            deployment_id,
+            model_id,
+            feature_data=reporting_data["_features_df"],
+            predictions=reporting_data["_predictions"],
+            association_ids=reporting_data["_association_ids"],
+            class_names=reporting_data["_class_names"],
+            request_parameters=reporting_data["_request_parameters"],
+            forecast_distance=reporting_data["_forecast_distance"],
+            row_index=reporting_data["_row_indexes"],
+            partition=reporting_data["_partition"],
+            series_id=reporting_data["_series_id"],
             skip_drift_tracking=skip_drift_tracking,
             skip_accuracy_tracking=skip_accuracy_tracking,
         )
 
+    def _deep_copy_reporting_data(
+        self,
+        features_df=None,
+        predictions=None,
+        association_ids=None,
+        forecast_distance=None,
+        row_index=None,
+        partition=None,
+        series_id=None,
+        class_names=None,
+        request_parameters=None,
+    ):
+        reporting_data = {
+            key: None
+            for key in [
+                "_features_df",
+                "_predictions",
+                "_association_ids",
+                "_forecast_distance",
+                "_row_indexes",
+                "_partition",
+                "_series_id",
+                "_class_names",
+                "_request_parameters",
+            ]
+        }
+        # Validate and modify request parameters
+        if request_parameters:
+            reporting_data["_request_parameters"] = self._update_request_parameters(
+                request_parameters
+            )
+        if features_df is not None:
+            reporting_data["_features_df"] = features_df.copy(deep=True)
+            # Reseting index is required to drop exact rows for which predictions are NaN
+            reporting_data["_features_df"].reset_index(drop=True, inplace=True)
+        if predictions is not None:
+            reporting_data["_predictions"] = copy.deepcopy(predictions)
+        if association_ids is not None:
+            reporting_data["_association_ids"] = copy.deepcopy(association_ids)
+        if forecast_distance is not None:
+            reporting_data["_forecast_distance"] = copy.deepcopy(forecast_distance)
+        if row_index is not None:
+            reporting_data["_row_indexes"] = copy.deepcopy(row_index)
+        if partition is not None:
+            reporting_data["_partition"] = copy.deepcopy(partition)
+        if series_id is not None:
+            reporting_data["_series_id"] = copy.deepcopy(series_id)
+        if class_names is not None:
+            reporting_data["_class_names"] = copy.deepcopy(class_names)
+
+        return reporting_data
+
+    @staticmethod
+    def _remove_nans(nan_prediction_indexes, reporting_data):
+        if not nan_prediction_indexes:
+            return
+
+        if reporting_data["_features_df"] is not None:
+            # In case of Time Series Predictions, where number of feature rows == number of predictions
+            # we can eliminate the rows easily
+            if reporting_data["_features_df"].shape[0] == len(reporting_data["_predictions"]):
+                reporting_data["_features_df"].drop(nan_prediction_indexes, axis=0, inplace=True)
+
+            # In case of Time Series Predictions, where number of feature rows != number of predictions
+            # it will be really hard to identify which rows to delete in case of NaN predictions.  So,
+            # we will not remove any rows from features_df.  That is ok, because number of feature rows are
+            # anyways not equal to the number of predictions.
+
+        # Need to remove invalid indexes in reverse order, or else it will mess up all the indexes
+        for invalid_index in sorted(nan_prediction_indexes, reverse=True):
+            del reporting_data["_predictions"][invalid_index]
+            if reporting_data["_association_ids"]:
+                del reporting_data["_association_ids"][invalid_index]
+            if reporting_data["_forecast_distance"]:
+                del reporting_data["_forecast_distance"][invalid_index]
+            if reporting_data["_row_indexes"]:
+                del reporting_data["_row_indexes"][invalid_index]
+            if reporting_data["_partition"]:
+                del reporting_data["_partition"][invalid_index]
+            if reporting_data["_series_id"]:
+                del reporting_data["_series_id"][invalid_index]
+
+        return reporting_data
+
     def report_event(self, deployment_id, model_id, event):
         """
         Wrap event in a container and use report_stats() to place in queue.
         """
         # automatically set deployment ID so user's code doesn't need to
         if event.is_entity_a_deployment():
             event.set_entity_id(deployment_id)
@@ -741,31 +885,31 @@
         )
         predictions_data_container = PredictionsDataContainer(
             self._get_general_stats(model_id, batch_name=batch_name), predictions_data
         )
         self._report_stats(deployment_id, model_id, predictions_data_container)
 
     def _validate_time_series_prediction_report(
-        self, predictions, forecast_distance, row_index, partition, series_id
+        self, predictions, forecast_distance, row_index, partition
     ):
         self._validate_parameter(predictions, forecast_distance, "forecast distance", int)
         self._validate_parameter(predictions, row_index, "row index", int)
         self._validate_parameter(predictions, partition, "partition", datetime)
 
     @staticmethod
     def _validate_parameter(predictions, parameter, param_name, expected_type):
         if not parameter:
             raise DRCommonException(
-                "'{}' values are required to report time series predictions".format(parameter)
+                f"'{parameter}' values are required to report time series predictions"
             )
         if not isinstance(parameter, list):
-            raise DRUnsupportedType("{} argument has to be of type '{}'".format(param_name, list))
+            raise DRUnsupportedType(f"{param_name} argument has to be of type '{list}'")
         if len(predictions) != len(parameter):
             raise DRCommonException(
-                "Number of predictions and {} values should be the same".format(param_name)
+                f"Number of predictions and {param_name} values should be the same"
             )
         for param in parameter:
             if not isinstance(param, expected_type):
                 raise DRCommonException(
                     "Value {} is of type {}, expected of type {}".format(
                         param, type(param), expected_type
                     )
@@ -778,15 +922,15 @@
         if not isinstance(series_id, list):
             raise DRUnsupportedType("'series_id' argument has to be of type 'list'")
 
         # If all values in the series are None, then simply convert series id to be None
         if all(_id is None for _id in series_id):
             return None
 
-        self._validate_parameter(predictions, series_id, "series id", string_types)
+        self._validate_parameter(predictions, series_id, "series id", str)
         return series_id
 
     def _update_request_parameters(self, request_parameters):
         allowed_keys = self.REQUEST_PARAMETERS_MAPPING.keys()
         camel_case_values = self.REQUEST_PARAMETERS_MAPPING.values()
         updated_request_parameters = {}
         for key, value in request_parameters.items():
@@ -818,7 +962,17 @@
             with open(features_types_filename, "rb") as f:
                 feature_types = json.load(f)
         if features_types_json:
             feature_types = json.loads(features_types_json)
 
         validate_feature_types(feature_types)
         return convert_dict_to_feature_types(feature_types)
+
+    def _report_external_nan_predictions_event(
+        self, deployment_id, model_id, nan_prediction_indexes
+    ):
+        # Generate ExternalNaNPredictions event
+        external_nan_predictions_event = ExternalNaNPredictionsEvent(
+            deployment_id, model_id, nan_prediction_indexes
+        )
+
+        self.report_event(deployment_id, model_id, external_nan_predictions_event)
```

## datarobot/mlops/channel/output_channel_queue.py

```diff
@@ -1,25 +1,18 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import abc
 import atexit
 import logging
 import time
-from builtins import dict
 from multiprocessing import Event
 from multiprocessing import Process
 from multiprocessing import Queue
 from multiprocessing import Value
 from queue import Empty
 from queue import Full
 
-import datarobot.mlops.install_aliases  # noqa: F401
 from datarobot.mlops.channel.record import Record
 from datarobot.mlops.common import config
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.enums import DataFormat
 from datarobot.mlops.common.enums import DataType
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.exception import DRCommonException
@@ -36,15 +29,15 @@
 #  such source code.
 #
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
 
 
-class OutputChannelQueue(object):
+class OutputChannelQueue:
     __metaclass__ = abc.ABCMeta
 
     TIMESTAMP = SerializationConstants.GeneralConstants.TIMESTAMP_FIELD_NAME
     MODEL_ID = SerializationConstants.GeneralConstants.MODEL_ID_FIELD_NAME
     FEATURES = SerializationConstants.PredictionsDataConstants.FEATURES_FIELD_NAME
     PREDICTIONS = SerializationConstants.PredictionsDataConstants.PREDICTIONS_FIELD_NAME
     ASSOCIATION_IDS = SerializationConstants.PredictionsDataConstants.ASSOCIATION_IDS_FIELD_NAME
@@ -275,17 +268,15 @@
 
     DEFAULT_REPORT_QUEUE_MAX_SIZE = 512 * 1024 * 1024  # 512MB
     DEFAULT_TIMEOUT_PROCESS_QUEUE_MS = 1000
     DEFAULT_QUEUE_OPERATION_TIMEOUT_SEC = 0.1
     DEFAULT_WORKER_TIMEOUT_SEC = 10
 
     def __init__(self):
-        self._logger = logging.getLogger(
-            "{}.{}".format(self.__class__.__module__, self.__class__.__name__)
-        )
+        self._logger = logging.getLogger(f"{self.__class__.__module__}.{self.__class__.__name__}")
         self._output_channel = None
 
         # set timeouts
         timeout_process_queue_msec = config.get_config_default(
             ConfigConstants.TIMEOUT_PROCESS_QUEUE_MS, self.DEFAULT_TIMEOUT_PROCESS_QUEUE_MS
         )
         self._timeout_process_queue_sec = timeout_process_queue_msec / 1000
@@ -334,15 +325,15 @@
         # Enqueue 'END' to mark the queue end to the background process
         if self._worker.is_alive():
             self._logger.debug("Adding END to queue")
             self._worker_queue.put(("END", None))
 
             # Wait for the background process to empty the queue
             endtime = time.time() + timeout_sec
-            self._logger.debug("Queue shutdown. Timeout is {}".format(timeout_sec))
+            self._logger.debug(f"Queue shutdown. Timeout is {timeout_sec}")
             num_waits = 0
             while self._worker_ready_event.is_set() and self._queue_current_size_bytes.value > 0:
                 time.sleep(self._timeout_process_queue_sec)
                 num_waits += 1
                 if num_waits % 10:
                     self._logger.info(
                         "Shutting down background reporting queue {} bytes remaining.".format(
@@ -363,15 +354,15 @@
             self._worker_ready_event.clear()
             self._logger.debug("Set process_records to False.")
 
             # If the user provided no timeout, wait patiently for the join to complete.
             # Otherwise, wait the longer of the two: the remaining timeout or a minimum.
             if timeout_sec > 0:
                 wait = max(endtime - time.time(), self.DEFAULT_WORKER_TIMEOUT_SEC)
-                self._logger.debug("Waiting for join {}".format(wait))
+                self._logger.debug(f"Waiting for join {wait}")
                 self._worker.join(wait)
             else:
                 self._logger.debug("Waiting for join for as long as it takes")
                 self._worker.join()
             self._logger.debug("Join completed")
 
         # If this is the final exit call, close the queue and terminate the worker.
```

## datarobot/mlops/channel/record.py

```diff
@@ -1,32 +1,24 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import binascii
 import struct
 
 #  Copyright (c) 2019 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
 #
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
-from builtins import bytes
-from builtins import str
 from random import getrandbits
 from uuid import UUID
 
-import datarobot.mlops.install_aliases  # noqa: F401
 from datarobot.mlops.common.enums import DataFormat
 from datarobot.mlops.common.enums import DataType
 from datarobot.mlops.common.exception import DRSpoolerException
 from datarobot.mlops.constants import Constants
 from datarobot.mlops.json_shim import json_dumps_str
 from datarobot.mlops.json_shim import json_loads
 
@@ -42,15 +34,15 @@
 
 
 def uuid4_fast():
     # https://bugs.python.org/issue45556
     return UUID(int=getrandbits(128), version=4)
 
 
-class RecordHeader(object):
+class RecordHeader:
     """
     Pack format rules:
     ! - network byte order (big-endian)
     I - unsigned int
     {}s - will be formatted with actual deployment id length
     e.g. 5s - 5 symbol string
     3I - three unsigned ints
@@ -219,15 +211,15 @@
 
     def __str__(self):
         return "deployment-id: {}, data-format: {}, data-type: {}, payload-len: {}".format(
             self._deployment_id, self._data_format, self._data_type, self._data_len
         )
 
 
-class Record(object):
+class Record:
     def __init__(
         self,
         deployment_id,
         data_type,
         data_format,
         payload,
         version=MLOPS_RECORD_VERSION,
@@ -238,17 +230,15 @@
         if data_format == DataFormat.BYTE_ARRAY:
             isinstance(payload, (bytearray, bytes))
             data_len = len(payload)
         elif data_format == DataFormat.JSON:
             # This isn't needed for this format and would be expensive to compute.
             data_len = None
         else:
-            raise DRSpoolerException(
-                "Record class does not support data format {}".format(data_format)
-            )
+            raise DRSpoolerException(f"Record class does not support data format {data_format}")
         self._record_header = RecordHeader(
             deployment_id,
             data_type,
             data_format,
             data_len,
             version=version,
             language=language,
@@ -342,21 +332,21 @@
         offset = 0
         (record_header_len,) = struct.unpack_from(
             DESERIALIZE_FORMAT_STRING_LEN, byte_array, offset=offset
         )
 
         offset += struct.calcsize(DESERIALIZE_FORMAT_STRING_LEN)
         (record_header_bytearray,) = struct.unpack_from(
-            "{}s".format(record_header_len), byte_array, offset=offset
+            f"{record_header_len}s", byte_array, offset=offset
         )
 
         record_header = RecordHeader.deserialize(record_header_bytearray)
         offset += record_header_len
         (payload_bytes_or_bytearray,) = struct.unpack_from(
-            "{}s".format(record_header.get_data_len()), byte_array, offset=offset
+            f"{record_header.get_data_len()}s", byte_array, offset=offset
         )
 
         return Record(
             record_header.get_deployment_id(),
             record_header.get_data_type(),
             record_header.get_data_format(),
             payload_bytes_or_bytearray,
@@ -398,9 +388,9 @@
         return self._payload
 
     def get_data_type(self):
         return self._record_header.get_data_type()
 
     def __str__(self):
         if isinstance(self._payload, (bytearray, bytes)):
-            return "{}, payload: {}".format(self._record_header, binascii.hexlify(self._payload))
-        return "{}, payload: {}".format(self._record_header, self._payload)
+            return f"{self._record_header}, payload: {binascii.hexlify(self._payload)}"
+        return f"{self._record_header}, payload: {self._payload}"
```

## datarobot/mlops/common/aggregation_util.py

```diff
@@ -1,16 +1,11 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 from collections import defaultdict
 
 import numpy as np
-import six
+
 from datarobot.mlops.common.exception import DRApiException
 from datarobot.mlops.common.stringutil import camelize
 from datarobot.mlops.metric import AggregatedStats
 from datarobot.mlops.metric import SerializationConstants
 
 #  Copyright (c) 2021 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
@@ -45,15 +40,15 @@
     )
 
 
 def _aggregated_stat_to_dict(aggregated_stat):
     from datarobot.mlops.stats_aggregator.histogram import CentroidHistogram
 
     return_dict = dict()
-    for k1, v1 in six.iteritems(aggregated_stat._asdict()):
+    for k1, v1 in aggregated_stat._asdict().items():
         if isinstance(v1, np.int64):
             return_dict[camelize(k1)] = int(v1)
         elif isinstance(v1, np.floating):
             return_dict[camelize(k1)] = float(v1)
         elif isinstance(v1, CentroidHistogram):
             # convert histogram to spooler format
             return_dict[camelize(k1)] = {
@@ -65,31 +60,31 @@
         else:
             return_dict[camelize(k1)] = v1
 
     return return_dict
 
 
 def _convert_stats_to_dict(aggregated_stats):
-    return {k: _aggregated_stat_to_dict(v) for k, v in six.iteritems(aggregated_stats)}
+    return {k: _aggregated_stat_to_dict(v) for k, v in aggregated_stats.items()}
 
 
 def _convert_predictions_stats_to_dict(prediction_stats, class_names):
     return_dict = {}
     if class_names is None:
         class_names = ["0"]  # Regression
 
     for class_name, prediction_stat in zip(class_names, prediction_stats):
         return_dict[class_name] = _aggregated_stat_to_dict(prediction_stat)
     return return_dict
 
 
 def _convert_segments_attributes_stats_to_dict(segment_attributes_stats, class_names):
     return_dict = defaultdict(dict)
-    for attribute_name, values in six.iteritems(segment_attributes_stats):
-        for value_name, agg_stats in six.iteritems(values):
+    for attribute_name, values in segment_attributes_stats.items():
+        for value_name, agg_stats in values.items():
             return_dict[attribute_name][value_name] = {
                 agg_stats_data_keys.NUMERIC_AGGREGATE_MAP: _convert_stats_to_dict(
                     agg_stats.get("numeric_stats")
                 ),
                 agg_stats_data_keys.CATEGORICAL_AGGREGATE_MAP: _convert_stats_to_dict(
                     agg_stats.get("categorical_stats")
                 ),
@@ -131,15 +126,15 @@
             "missingCount": missing_count,
         },
     }
 
 
 def _convert_to_category_stat(feature_name, stat):
     categories, counts = list(), list()
-    for category, count in six.iteritems(stat["categoryCounts"]):
+    for category, count in stat["categoryCounts"].items():
         categories.append(category)
         counts.append(count)
 
     return {
         "name": feature_name,
         "stats": {
             "count": stat["count"],
@@ -151,41 +146,41 @@
         },
     }
 
 
 def convert_aggregated_stats_features_to_dr_format(numeric_stat=None, category_stat=None):
     feature_list = list()
     if numeric_stat:
-        for feature_name, stat in six.iteritems(numeric_stat):
+        for feature_name, stat in numeric_stat.items():
             feature_list.append(_convert_to_numeric_stat(feature_name, stat))
 
     if category_stat:
-        for feature_name, stat in six.iteritems(category_stat):
+        for feature_name, stat in category_stat.items():
             feature_list.append(_convert_to_category_stat(feature_name, stat))
 
     return feature_list
 
 
 def convert_aggregated_stats_predictions_to_dr_format(prediction_stats=None):
     prediction_list = list()
     if prediction_stats:
-        for _, stat in six.iteritems(prediction_stats):
+        for _, stat in prediction_stats.items():
             prediction_list.append(_convert_stat_format(stat))
     return prediction_list
 
 
 def convert_aggregated_stats_segment_attr_to_dr_format(segment_attributes_stats=None):
     if (not segment_attributes_stats) or ("segmentStatsMap" not in segment_attributes_stats):
         return None
 
     segment_attributes_stats_map = segment_attributes_stats["segmentStatsMap"]
     segment_list = list()
-    for attribute_name, values in six.iteritems(segment_attributes_stats_map):
+    for attribute_name, values in segment_attributes_stats_map.items():
         segment_attr = list()
-        for value_name, stats in six.iteritems(values):
+        for value_name, stats in values.items():
             segment_attr.append(
                 {
                     "value": value_name,
                     "features": convert_aggregated_stats_features_to_dr_format(
                         stats.get(agg_stats_data_keys.NUMERIC_AGGREGATE_MAP),
                         stats.get(agg_stats_data_keys.CATEGORICAL_AGGREGATE_MAP),
                     ),
```

## datarobot/mlops/common/config.py

```diff
@@ -6,30 +6,20 @@
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
 #
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
 
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
 
 import logging
 import os
 import warnings
-from builtins import dict
-from builtins import int
-from builtins import object
-from builtins import str
 from collections import namedtuple
 
-import datarobot.mlops.install_aliases  # noqa: F401
-import six
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.exception import DRCommonException
 from datarobot.mlops.common.exception import DRConfigKeyAlreadyAssigned
 from datarobot.mlops.common.exception import DRConfigKeyNotFound
 from datarobot.mlops.common.exception import DRInvalidValue
 from datarobot.mlops.common.exception import DRUnsupportedType
 from datarobot.mlops.common.exception import DRVarNotFound
@@ -97,30 +87,30 @@
             return spooler_name
         except ValueError:
             # check whether this is an old name for a spooler type
             if spooler_name in spoolerRemap:
                 spooler_name = spoolerRemap.get(spooler_name)
                 return spooler_name
             else:
-                raise DRInvalidValue("Invalid value for spooler_type: {}".format(value))
+                raise DRInvalidValue(f"Invalid value for spooler_type: {value}")
 
     if config_key.type == bool:
         return True if value.lower() in ["true", "yes", "1"] else False
 
     elif config_key.type == int:
         return int(value) if value else 0
 
     elif config_key.type == float:
         return float(value) if value else 0.0
 
     elif config_key.type == str:
         return value
 
     else:
-        raise DRUnsupportedType("Unsupported config variable type: {}".format(config_key.type))
+        raise DRUnsupportedType(f"Unsupported config variable type: {config_key.type}")
 
 
 def get_config_default(config_key, default_val):
     try:
         return get_config(config_key)
     except DRConfigKeyNotFound:
         return default_val
@@ -140,21 +130,18 @@
         # verify this is a valid SPOOLER type
         spooler_name = value
         if spooler_name.upper() in spoolerRemap:
             spooler_name = spoolerRemap.get(spooler_name.upper())
         try:
             _ = SpoolerType.from_name(spooler_name)
         except ValueError:
-            raise DRInvalidValue("Invalid value for spooler_type: {}".format(value))
+            raise DRInvalidValue(f"Invalid value for spooler_type: {value}")
         value = spooler_name
 
-    # For Python 2.7 compatibility we need to treat the `str` type special.
-    required_type = config_key.type if config_key.type != str else six.string_types
-
-    if not isinstance(value, required_type):
+    if not isinstance(value, config_key.type):
         raise DRUnsupportedType(
             "Wrong value type for config key {}. Expected: {}, provided: {}".format(
                 config_key.name, config_key.type, type(value)
             )
         )
     __prioritized_config[config_key.name] = value
 
@@ -225,15 +212,15 @@
         config_key = ConfigConstants.get_config_key(key)
 
         if config_key is None:
             new_name = get_new_name(key)
             if new_name is not None:
                 config_key = ConfigConstants.get_config_key(new_name)
             if config_key is None:
-                raise DRVarNotFound("Invalid parameter '{}'".format(key))
+                raise DRVarNotFound(f"Invalid parameter '{key}'")
 
         if config_key.type == bool:
             value = True if value.lower() in ["true", "yes", "1"] else False
         elif config_key.type == int:
             value = int(value) if value else 0
         elif config_key.type == float:
             value = float(value) if value else 0.0
@@ -264,21 +251,21 @@
                 config_key.name, prev_value, value
             )
         )
 
     set_config(config_key, value)
 
 
-class ConfigKeys(object):
+class ConfigKeys:
     # string definitions
     MLOPS_FEATURE_DATA_ROWS_IN_ONE_MESSAGE_STR = "MLOPS_FEATURE_DATA_ROWS_IN_ONE_MESSAGE"
     MLOPS_CUSTOM_METRIC_ROWS_IN_ONE_MESSAGE_STR = "MLOPS_CUSTOM_METRIC_ROWS_IN_ONE_MESSAGE"
 
 
-class ConfigConstants(object):
+class ConfigConstants:
 
     MLOPS_SERVICE_URL = ConfigKey("MLOPS_SERVICE_URL", str)
     MLOPS_API_TOKEN = ConfigKey("MLOPS_API_TOKEN", str)
     MLOPS_MONITORING_AGENT_JAR_PATH = ConfigKey("MLOPS_MONITORING_AGENT_JAR_PATH", str)
 
     MLOPS_MAX_FEATURES_TO_MONITOR = ConfigKey("MLOPS_MAX_FEATURES_TO_MONITOR", int)
     MLOPS_SAMPLES_GROUP_SIZE_FOR_MLOPS_API_ENDPOINT = ConfigKey(
```

## datarobot/mlops/common/enums.py

```diff
@@ -1,17 +1,9 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
-from builtins import str
 from enum import IntEnum
 
-import datarobot.mlops.install_aliases  # noqa: F401
-
 #  Copyright (c) 2019 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
@@ -31,17 +23,15 @@
 
     @classmethod
     def from_name(cls, name):
         name = name.upper()
         for n, member in cls.__members__.items():
             if name == n:
                 return member
-        raise ValueError(
-            "'{}' name not found, allowed values: {}".format(name, str(cls.__members__.items()))
-        )
+        raise ValueError(f"'{name}' name not found, allowed values: {str(cls.__members__.items())}")
 
 
 class SpoolerType(EnumOrdinal):
     """
     Configures target location for output from MLOps library.
     """
 
@@ -68,15 +58,15 @@
     FEATURE_DRIFT = 1
     TARGET_DRIFT = 2
     DEPLOYMENT_STATS = 3
     FEATURE_DATA = 4
     PREDICTIONS_STATS_SC_CLASSIFICATION = 5
     PREDICTIONS_STATS_SC_REGRESSION = 6
     PREDICTIONS_DATA = 7
-    EXTERNAL_EVENT = 8
+    EVENT = 8
     ACTUALS_DATA = 9
     PREDICTION_STATS = 10
     CUSTOM_METRIC = 11
 
 
 class PredictionType(EnumOrdinal):
     REGRESSION = 0
```

## datarobot/mlops/common/exception.py

```diff
@@ -1,14 +1,7 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
-import datarobot.mlops.install_aliases  # noqa: F401
-
 #  Copyright (c) 2019 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
```

## datarobot/mlops/common/prediction_util.py

```diff
@@ -1,16 +1,8 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import copy
-from builtins import str
-
-import datarobot.mlops.install_aliases  # noqa: F401
 
 #  Copyright (c) 2021 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
```

## datarobot/mlops/common/singleton.py

```diff
@@ -1,16 +1,9 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 from functools import wraps
 
-import datarobot.mlops.install_aliases  # noqa: F401
-
 #  Copyright (c) 2019 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
```

## datarobot/mlops/common/stringutil.py

```diff
@@ -1,14 +1,7 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
-import datarobot.mlops.install_aliases  # noqa: F401
-
 #  Copyright (c) 2019 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
```

## datarobot/mlops/spooler/async_memory_spooler.py

```diff
@@ -1,28 +1,18 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import multiprocessing
 import sys
 import time
-from builtins import range
-from builtins import str
-from builtins import super
 from multiprocessing import Process
 from multiprocessing import Queue
 from multiprocessing import Value
 from multiprocessing.connection import Client
 from multiprocessing.connection import Listener
 from queue import Empty
 from urllib.parse import urlparse
 
-import datarobot.mlops.install_aliases  # noqa: F401
-import six
 from datarobot.mlops.channel.record import Record
 from datarobot.mlops.common import config
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.enums import MLOpsSpoolAction
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.exception import DRSpoolerException
 from datarobot.mlops.spooler.record_spooler import RecordSpooler
@@ -37,33 +27,32 @@
 #
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
 
 
 # Known issues with 'spawn' on python-3.7+, hence forcing the fork
-if six.PY3:
-    try:
-        multiprocessing.set_start_method("fork")
-    except RuntimeError:
-        # If it fails, it is most likely that it is already set correctly
-        # so ignore it and move on
-        pass
+try:
+    multiprocessing.set_start_method("fork")
+except RuntimeError:
+    # If it fails, it is most likely that it is already set correctly
+    # so ignore it and move on
+    pass
 
 
 class AsyncMemoryRecordSpooler(RecordSpooler):
     DEFAULT_MAX_QUEUE_SIZE = 1024 * 1024 * 1024  # 1G
     MAX_RECORDS_TO_DEQUEUE = 10
     DEFAULT_QUEUE_OPERATION_TIMEOUT = 0.1
 
     DEFAULT_URL = "http://localhost:6000"
     DEFAULT_START_AS_SERVER = False
 
     def __init__(self, start_as_server=False):
-        super(AsyncMemoryRecordSpooler, self).__init__()
+        super().__init__()
 
         self._queue = None
         self._queue_size = None
         self._max_queue_size = self.DEFAULT_MAX_QUEUE_SIZE
         self._initialized = False
 
         self._server = None
@@ -102,17 +91,17 @@
             while True:
                 msg = self._connection.recv()
                 message_size = sys.getsizeof(msg)
                 if queue_size.value < (self._max_queue_size - message_size):
                     queue.put(msg)
                     with queue_size.get_lock():
                         queue_size.value += message_size
-                    self._logger.debug("Enqueue message of size {}".format(message_size))
+                    self._logger.debug(f"Enqueue message of size {message_size}")
         except EOFError as e:
-            self._logger.warning("client was closed - {}".format(e))
+            self._logger.warning(f"client was closed - {e}")
 
     def open(self, action=MLOpsSpoolAction.ENQUEUE):
         if not self._initialized:
             self.set_config()
             if self._start_as_server:
                 self._queue = Queue()
                 self._queue_size = Value("i", 0)
@@ -154,40 +143,40 @@
     def enqueue_single_message(self, message_json):
         if not self._initialized:
             raise DRSpoolerException("Spooler must be opened before using.")
 
         # Check size limit
         message_size = sys.getsizeof(message_json)
         if message_size > self.get_message_byte_size_limit():
-            raise DRSpoolerException("Cannot enqueue record size: {}".format(message_size))
+            raise DRSpoolerException(f"Cannot enqueue record size: {message_size}")
 
         self._connection.send(message_json)
 
     def enqueue(self, record_list):
         if not self._initialized:
             raise DRSpoolerException("Spooler must be opened before using.")
 
         if self._enqueue_delay_sec > 0:
             time.sleep(self._enqueue_delay_sec)
 
         for r in record_list:
             self.enqueue_single_message(r.to_json())
-        self._logger.debug("Published {} messages".format(len(record_list)))
+        self._logger.debug(f"Published {len(record_list)} messages")
 
     def dequeue_single_message(self):
         if not self._initialized:
             raise DRSpoolerException("Spooler must be opened before using.")
 
         if self._dequeue_delay_sec > 0:
             time.sleep(self._dequeue_delay_sec)
 
         try:
             message_json = self._queue.get(block=True, timeout=self.DEFAULT_QUEUE_OPERATION_TIMEOUT)
             message_size = sys.getsizeof(message_json)
-            self._logger.debug("Dequeue message of size {}".format(message_size))
+            self._logger.debug(f"Dequeue message of size {message_size}")
             with self._queue_size.get_lock():
                 self._queue_size.value -= message_size
             return message_json
         except Empty:
             return None
 
     def dequeue(self):
```

## datarobot/mlops/spooler/filesystem_spooler.py

```diff
@@ -1,31 +1,17 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import errno
 import glob
-import io
 import json
 import logging
 import os
 import struct
 import sys
 import time
 import zlib
-from builtins import bytes
-from builtins import int
-from builtins import map
-from builtins import object
-from builtins import str
-from builtins import super
 
-import datarobot.mlops.install_aliases  # noqa: F401
-import six
 from datarobot.mlops.channel.record import Record
 from datarobot.mlops.common import config
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.enums import DataFormat
 from datarobot.mlops.common.enums import MLOpsSpoolAction
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.exception import DRApiException
@@ -71,15 +57,15 @@
 
     DEFAULT_DEQUEUE_SPOOL_META_FILENAME = "spool_meta.consumer"
 
     DEFAULT_FILESYSTEM_ACK_RETRY = 3
     DEFAULT_FILESYSTEM_ACK_DEADLINE_SEC = 600  # 10 minutes
 
     def __init__(self):
-        super(FSRecordSpooler, self).__init__()
+        super().__init__()
 
         self._spool_directory_path = None
         self._spool_file_max_size = None
         self._spool_max_files = None
 
         # enqueue variables
         self._enqueue_initialized = False
@@ -124,17 +110,17 @@
         return [
             ConfigConstants.FILESYSTEM_MAX_NUM_FILES,
             ConfigConstants.FILESYSTEM_MAX_FILE_SIZE,
             ConfigConstants.SPOOLER_CHECKSUM,
         ]
 
     def set_config(self):
-        missing = super(FSRecordSpooler, self).get_missing_config()
+        missing = super().get_missing_config()
         if len(missing) > 0:
-            raise DRSpoolerException("Configuration values missing: {}".format(missing))
+            raise DRSpoolerException(f"Configuration values missing: {missing}")
 
         self._spool_directory_path = os.path.abspath(
             config.get_config(ConfigConstants.FILESYSTEM_DIRECTORY)
         )
         self._spool_file_max_size = config.get_config_default(
             ConfigConstants.FILESYSTEM_MAX_FILE_SIZE, self.DEFAULT_MAX_FILE_SIZE
         )
@@ -269,19 +255,17 @@
             try:
                 self._dequeue_files.remove(self._current_dequeue_file)
             except ValueError:
                 return
 
     def _print_dequeue_counters(self):
         if self._current_dequeue_file is not None:
-            self._logger.info(
-                "Dequeue counters for the spool file: {}".format(self._current_dequeue_file)
-            )
+            self._logger.info(f"Dequeue counters for the spool file: {self._current_dequeue_file}")
             for data_type, count in self._dequeue_counters.items():
-                self._logger.info("Data Type: {} Number of records: {}".format(data_type, count))
+                self._logger.info(f"Data Type: {data_type} Number of records: {count}")
 
     def _get_next_spool_file_to_dequeue(self):
         self._load_dequeue_spool_file_list()
 
         # No new dequeue file is present
         if len(self._dequeue_files) == 0:
             self._current_dequeue_file = None
@@ -290,16 +274,16 @@
         self._current_dequeue_file = self._dequeue_files[0]
         self._dequeue_files.remove(self._current_dequeue_file)
         self._dequeue_spool_data_format = self._get_data_format_of_spool_file(
             self._current_dequeue_file, self._dequeue_spool_data_format
         )
         self._validate_dequeue_format(self._dequeue_spool_data_format)
 
-        self._logger.info("Starting dequeue with new file: {}".format(self._current_dequeue_file))
-        self._dequeue_fd = io.open(self._current_dequeue_file, "r")
+        self._logger.info(f"Starting dequeue with new file: {self._current_dequeue_file}")
+        self._dequeue_fd = open(self._current_dequeue_file)
         self._dequeue_bytes_read = 0
         for data_type in self._dequeue_counters:
             self._dequeue_counters[data_type] = 0
         return True
 
     def _check_and_create_lock_for_dequeue(self):
         prefix = "agent"
@@ -314,31 +298,84 @@
                  process id [pid={}].  If no agent process is running, remove the file: '{}'
                  and then rerun the command""".format(
                     self._spool_directory_path, agent_pid, lock_files[0]
                 )
             )
         pid = os.getpid()
         self._dequeue_lock_file = os.path.join(
-            self._spool_directory_path, "{}_{}{}".format(prefix, pid, suffix)
+            self._spool_directory_path, f"{prefix}_{pid}{suffix}"
         )
         # Touch the lock file
         with open(self._dequeue_lock_file, "a"):
             os.utime(self._dequeue_lock_file, None)
 
+    def _get_current_enqueue_pointer(self):
+        spool_files_paths = self._read_spool_dir()
+        spool_files = sorted(spool_files_paths, key=self.key_modification_time)
+        if len(spool_files) == 0:
+            return None, None
+        last_spool_file = spool_files[-1]
+        stats = os.stat(last_spool_file)
+        offset = stats.st_size
+        file_index = max(
+            list(map(lambda x: FSRecordSpooler._spool_file_index_get(x), spool_files_paths))
+        )
+        return file_index, offset
+
+    def _verify_enqueue_pointer_ahead_of_dequeue_pointer(self):
+        last_spool_file = os.path.join(
+            self._spool_directory_path, self._dequeue_metafile.get_last_spool_file_name()
+        )
+        dequeue_file_index = self._spool_file_index_get(last_spool_file)
+        dequeue_file_offset = self._dequeue_metafile.get_offset()
+
+        enqueue_file_index, enqueue_file_offset = self._get_current_enqueue_pointer()
+        if enqueue_file_index is None or enqueue_file_offset is None:
+            raise DRSpoolerException(
+                "Missing spool files but meta file exists({}, {}), Possible spooler"
+                " corruption that needs to be fixed manually".format(
+                    self._dequeue_metafile.get_last_spool_file_name(),
+                    dequeue_file_offset,
+                )
+            )
+
+        if enqueue_file_index < dequeue_file_index or (
+            enqueue_file_index == dequeue_file_index and enqueue_file_offset < dequeue_file_offset
+        ):
+            enqueue_file = os.path.join(
+                self._spool_directory_path,
+                self.FS_SPOOLER_FILENAME + str(enqueue_file_index),
+            )
+            raise DRSpoolerException(
+                "Enqueue pointer ({}, {}) is behind Dequeue pointer ({}, {}), Possible spooler"
+                " corruption that needs to be fixed manually".format(
+                    enqueue_file,
+                    enqueue_file_offset,
+                    self._dequeue_metafile.get_last_spool_file_name(),
+                    dequeue_file_offset,
+                )
+            )
+
     def _init_dequeue(self):
         self._check_and_create_lock_for_dequeue()
 
         self._load_meta_file_for_dequeue()
         if self._dequeue_metafile.exists():
+            try:
+                self._verify_enqueue_pointer_ahead_of_dequeue_pointer()
+            except DRSpoolerException:
+                self._remove_dequeue_lock_file()
+                raise
             last_spool_file = os.path.join(
                 self._spool_directory_path, self._dequeue_metafile.get_last_spool_file_name()
             )
             if not os.path.exists(last_spool_file):
                 self._get_next_spool_file_to_dequeue()
                 return
+
             dequeue_spool_data_format = self._get_data_format_of_spool_file(
                 last_spool_file, self._dequeue_spool_data_format
             )
             self._validate_dequeue_format(dequeue_spool_data_format)
             if self._dequeue_end_of_file_reached(last_spool_file):
                 self._dequeue_delete_spool_file(last_spool_file)
                 self._get_next_spool_file_to_dequeue()
@@ -346,15 +383,15 @@
                 self._current_dequeue_file = last_spool_file
                 self._dequeue_bytes_read = self._dequeue_metafile.get_offset()
                 self._logger.info(
                     "Starting dequeue from file '{}' @ offset: {}".format(
                         self._current_dequeue_file, self._dequeue_bytes_read
                     )
                 )
-                self._dequeue_fd = io.open(self._current_dequeue_file, "r")
+                self._dequeue_fd = open(self._current_dequeue_file)
                 self._dequeue_fd.seek(self._dequeue_bytes_read)
                 self._dequeue_spool_data_format = dequeue_spool_data_format
         else:
             self._get_next_spool_file_to_dequeue()
         self._dequeue_initialized = True
 
     def _init_enqueue(self):
@@ -383,48 +420,48 @@
         else:
             self._close_current_and_request_new_enqueue_file()
         self._enqueue_initialized = True
 
     def _get_data_format_of_spool_file(self, spool_filepath, default):
         if os.stat(spool_filepath).st_size == 0:
             return default
-        with io.open(spool_filepath, "rb") as f:
+        with open(spool_filepath, "rb") as f:
             magic = struct.unpack_from(
                 "!1I", f.read(FSRecordSpooler.MAGIC_RECORD_DELIMITER_SIZE_BYTES), 0
             )[0]
             if magic == FSRecordSpooler.MAGIC_NUMBER_RECORD:
                 return DataFormat.BYTE_ARRAY
             else:
                 return DataFormat.JSON
 
     def _init_enqueue_json(self, enqueue_filepath):
         # if last file still has some space, then use it
         try:
             stat = os.stat(enqueue_filepath)
             if stat.st_size < self._spool_file_max_size:
-                self._enqueue_file = io.open(enqueue_filepath, "a")
+                self._enqueue_file = open(enqueue_filepath, "a")
             else:
                 self._close_current_and_request_new_enqueue_file()
-        except IOError:
+        except OSError:
             pass
 
     def _init_enqueue_binary(self, enqueue_filepath):
         # if last file does not have LAST_RECORD_DELIMITER, continue writing into it
         try:
             if not self._file_has_last_record_delimiter(enqueue_filepath):
-                self._enqueue_file = io.open(enqueue_filepath, "ab")
+                self._enqueue_file = open(enqueue_filepath, "ab")
             else:
                 self._close_current_and_request_new_enqueue_file()
-        except IOError:
+        except OSError:
             pass
 
     # used for enqueue
     @staticmethod
     def _file_has_last_record_delimiter(filename):
-        with io.open(filename, "rb") as f:
+        with open(filename, "rb") as f:
             """
             seek(offset, [whence]) params are:
             1st - offset calculated from the 2nd param;
             2nd - whence: 0-absolute position; 1-current position, 2-end.
             """
             f.seek(0, 2)
             # empty file case
@@ -456,32 +493,35 @@
     def _read_spool_dir(self):
         return glob.glob(
             os.path.join(self._spool_directory_path, FSRecordSpooler.FS_SPOOLER_FILENAME + "*")
         )
 
     def _close_current_enqueue(self, filename, format):
         if format == DataFormat.BYTE_ARRAY:
-            with io.open(filename, "ab") as f:
+            with open(filename, "ab") as f:
                 f.write(
                     struct.pack(
                         FSRecordSpooler.PACK_FORMAT_INTEGER,
                         FSRecordSpooler.LAST_RECORD_DELIMITER,
                     )
                 )
 
-    def close(self):
-        self._close_enqueue_file()
-        if self._dequeue_fd is not None:
-            self._dequeue_fd.close()
+    def _remove_dequeue_lock_file(self):
         if self._dequeue_lock_file is not None:
             try:
                 os.remove(self._dequeue_lock_file)
             except OSError as e:
                 if e.errno != errno.ENOENT:
                     raise
+
+    def close(self):
+        self._close_enqueue_file()
+        if self._dequeue_fd is not None:
+            self._dequeue_fd.close()
+        self._remove_dequeue_lock_file()
         self._print_dequeue_counters()
 
     def _close_enqueue_file(self):
         if self._enqueue_file is not None:
             self._enqueue_file.close()
             self._enqueue_file = None
 
@@ -502,17 +542,17 @@
             )
         self._enqueue_file_index += 1
         new_enqueue_filepath = os.path.join(
             self._spool_directory_path,
             self.FS_SPOOLER_FILENAME + str(self._enqueue_file_index),
         )
         if self._spool_data_format == DataFormat.BYTE_ARRAY:
-            self._enqueue_file = io.open(new_enqueue_filepath, "ab")
+            self._enqueue_file = open(new_enqueue_filepath, "ab")
         else:
-            self._enqueue_file = io.open(new_enqueue_filepath, "a")
+            self._enqueue_file = open(new_enqueue_filepath, "a")
 
     def _enqueue_check_spooler_has_enough_space(self, bytes_to_write_size):
         if (
             self._enqueue_current_size
             + bytes_to_write_size
             + FSRecordSpooler.LAST_RECORD_DELIMITER_SIZE_BYTES
             > self._spool_capacity
@@ -563,15 +603,16 @@
             raise DRSpoolerException(
                 "Fewer bytes has been written than expected: written: {}; expected: {}.".format(
                     bytes_written, buf_len
                 )
             )
         self._enqueue_current_size += buf_len
         if self._spool_data_format == DataFormat.JSON:
-            self._enqueue_file.write(six.text_type("\n"))
+            assert self._enqueue_file is not None
+            self._enqueue_file.write("\n")
             self._enqueue_current_size += 1
 
     def enqueue_single_record(self, record):
         """
         Enqueue Record object into spooler
 
         :param record: Record object to enqueue
@@ -610,15 +651,15 @@
                     record_len,
                     checksum,
                 )
             )
             buf.extend(serialized_record)
             self._enqueue_write_data(buf)
         else:
-            self._enqueue_write_data(six.text_type(serialized_record))
+            self._enqueue_write_data(serialized_record)
 
         # We need to explicitly issue a flush, otherwise the last Record stays
         # in buffer until file is closed. This is especially a problem with
         # the embedded agent case, where the agent keeps waiting for the record
         # in the spool file, which is not flushed until the program exits.
         if self._enqueue_file:
             self._enqueue_file.flush()
@@ -763,35 +804,35 @@
                 )
                 self._spooler_offset_manager.track_offset_record(
                     self._dequeue_bytes_read, record.get_id()
                 )
             record_list.append(record)
             return record_list
         except Exception as e:
-            self._logger.error("Exception during dequeue: {}".format(e))
+            self._logger.error(f"Exception during dequeue: {e}")
             return record_list
 
 
 # MetaFile used for dequeue
-class MetaFile(object):
+class MetaFile:
     def __init__(self, meta_filename):
         self._dequeue_meta_filename = meta_filename
         if not os.path.exists(meta_filename):
             self._exists = False
             self._dequeue_offset = 0
             self._last_dequeue_spool_filename = None
             return
 
         self._exists = True
 
-        with open(meta_filename, "r") as f:
+        with open(meta_filename) as f:
             metadata = json.loads(f.read())
 
         if metadata is None:
-            raise Exception("Invalid metadata in metafile: {}".format(meta_filename))
+            raise Exception(f"Invalid metadata in metafile: {meta_filename}")
 
         if "file_name" not in metadata:
             raise Exception("Invalid metafile format: 'file_name' key is missing")
 
         if "offset" not in metadata:
             raise Exception("Invalid metafile format: 'offset' key is missing")
```

## datarobot/mlops/spooler/kafka_spooler.py

```diff
@@ -1,49 +1,41 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import logging
 import time
-from builtins import dict
-from builtins import super
 from contextlib import contextmanager
 from enum import Enum
 
-import certifi
-import datarobot.mlops.install_aliases  # noqa: F401
-
 #  Copyright (c) 2021 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
 #  such source code.
 #
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
-import six
+from urllib.parse import urlparse
+
+import certifi
 from confluent_kafka import Consumer
 from confluent_kafka import KafkaError
 from confluent_kafka import Producer
+
 from datarobot.mlops.channel.record import Record
 from datarobot.mlops.common import config
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.enums import MLOpsSpoolAction
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.exception import DRCommonException
 from datarobot.mlops.common.exception import DRConfigKeyNotFound
 from datarobot.mlops.common.exception import DRSpoolerException
 from datarobot.mlops.constants import Constants
 from datarobot.mlops.spooler.filesystem_spooler import SpoolerOffsetManager
 from datarobot.mlops.spooler.record_spooler import RecordSpooler
-from future.moves.urllib.parse import urlparse
 
 logger = logging.getLogger(__name__)
 
 
 @contextmanager
 def suppress(exception_type):
     try:
@@ -83,15 +75,15 @@
     DEFAULT_KAFKA_CONSUMER_POLL_TIMEOUT_MS = 3000
     DEFAULT_KAFKA_CONSUMER_MAX_NUM_MESSAGES = 100
 
     KAFKA_DEFAULT_ACK_RETRY = 3
     KAFKA_DEFAULT_ACK_DEADLINE = 600
 
     def __init__(self):
-        super(KafkaRecordSpooler, self).__init__()
+        super().__init__()
         self._initialized = False
         self._kafka_producer_config = None
         self._kafka_consumer_config = None
         self._topic_name = None
         self._bootstrap_servers = None
         self._kafka_properties_location = None
         self._consumer = None
@@ -189,17 +181,20 @@
         with suppress(DRConfigKeyNotFound):
             sasl_mechanism = config.get_config(ConfigConstants.KAFKA_SASL_MECHANISM).upper()
             _config[KafkaConf.SASL_MECHANISM] = sasl_mechanism
             if sasl_mechanism == "OAUTHBEARER":
                 oauth_conf_str = config.get_config_default(
                     ConfigConstants.KAFKA_SASL_OAUTHBEARER_CONFIG, ""
                 )
+                # TODO: consider deprecating this param (users should set the Azure
+                # standard env vars, i.e. AZURE_TENANT_ID, ...)
                 _config[KafkaConf.SASL_OAUTHBEARER_CONFIG] = oauth_conf_str
                 oauth_handler = AzureActiveDirectoryOauthBearer.from_config_str(
-                    oauth_conf_str, self._bootstrap_servers
+                    self._bootstrap_servers,
+                    oauth_conf_str,
                 )
                 _config[KafkaConf.OAUTH_CALLBACK] = oauth_handler
 
         with suppress(DRConfigKeyNotFound):
             _config[KafkaConf.REQUEST_TIMEOUT] = config.get_config(
                 ConfigConstants.KAFKA_REQUEST_TIMEOUT_MS
             )
@@ -370,15 +365,15 @@
         else:
             raise DRSpoolerException("Unable to publish record due to full local buffer.")
         # used to trigger delivery report callbacks for previously delivered records
         self._producer.poll(0)
 
     def commit_next_valid_offset(self):
         offset_meta_map = self._spooler_offset_manager.find_next_offsets()
-        for partition, offset_meta_list in six.iteritems(offset_meta_map):
+        for partition, offset_meta_list in offset_meta_map.items():
             if len(offset_meta_list) == 0:
                 continue
 
             offset_meta_max = max(offset_meta_list, key=lambda item: item.get_offset())
             consumer_message = self._records_pending_ack.get(offset_meta_max.get_record_id())
             if consumer_message is not None:
                 self._consumer.commit(consumer_message)
@@ -487,74 +482,67 @@
             ConfigConstants.SPOOLER_TYPE.name: SpoolerType.KAFKA.name,
             ConfigConstants.KAFKA_TOPIC_NAME.name: self._topic_name,
             ConfigConstants.KAFKA_BOOTSTRAP_SERVERS.name: self._bootstrap_servers,
             ConfigConstants.KAFKA_MAX_FLUSH_MS.name: int(self._max_flush_time * 1000),
         }
 
 
-class AzureActiveDirectoryOauthBearer(object):
-    AUTHORITY_TEMPLATE = "https://login.microsoftonline.com/{}/"
+class AzureActiveDirectoryOauthBearer:
     AAD_TENANT_ID = "aad.tenant.id"
     AAD_CLIENT_ID = "aad.client.id"
     AAD_CLIENT_SECRET = "aad.client.secret"
 
-    def __init__(self, tenant_id, client_id, client_secret, scopes):
+    def __init__(self, scopes, tenant_id=None, client_id=None, client_secret=None):
         # Delay import since the dep is optional as only Azure Event Hubs users will want to use
         # this auth method.
         try:
-            import msal
+            from azure.identity import ClientSecretCredential
+            from azure.identity import DefaultAzureCredential
         except ImportError:
             message = (
                 "Azure Active Directory Authentication failed; missing package; "
                 "need to `pip install {}[azure]`".format(Constants.OFFICIAL_NAME)
             )
             raise RuntimeError(message)
 
-        authority = self.AUTHORITY_TEMPLATE.format(tenant_id)
-        self.app = msal.ConfidentialClientApplication(
-            client_id,
-            authority=authority,
-            client_credential=client_secret,
-        )
+        # If values were provided programmatically, assume we are using ClientSecret,
+        # otherwise use the default credential (which checks multiple places).
+        if tenant_id and client_id and client_secret:
+            self.cred = ClientSecretCredential(
+                client_id=client_id,
+                client_secret=client_secret,
+                tenant_id=tenant_id,
+            )
+        else:
+            self.cred = DefaultAzureCredential()
         self.scopes = scopes
 
     @classmethod
-    def from_config_str(cls, config_string, bootstrap_servers):
-        oauthbearer_config = dict(x.strip().split("=") for x in config_string.split(","))
+    def from_config_str(cls, bootstrap_servers, config_string=""):
+        if config_string:
+            oauthbearer_config = dict(x.strip().split("=") for x in config_string.split(","))
+        else:
+            oauthbearer_config = {}
         logger.debug("Parsed config str: %s", oauthbearer_config)
 
         _bootstrap_server = bootstrap_servers.split(",")[0].strip()
         logger.debug("Parsed bootstrap server: %s", _bootstrap_server)
         _uri = urlparse("https://" + _bootstrap_server)
-        scope = "{}://{}/.default".format(_uri.scheme, _uri.hostname)
+        scope = f"{_uri.scheme}://{_uri.hostname}/.default"
         logger.debug("Generated scope: %s", scope)
         return cls(
-            tenant_id=oauthbearer_config[cls.AAD_TENANT_ID],
-            client_id=oauthbearer_config[cls.AAD_CLIENT_ID],
-            client_secret=oauthbearer_config[cls.AAD_CLIENT_SECRET],
-            scopes=[scope],
+            tenant_id=oauthbearer_config.get(cls.AAD_TENANT_ID),
+            client_id=oauthbearer_config.get(cls.AAD_CLIENT_ID),
+            client_secret=oauthbearer_config.get(cls.AAD_CLIENT_SECRET),
+            scopes=scope,
         )
 
     def __call__(self, _config):
         """
         Note here value of _config comes from sasl.oauthbearer.config below.
         It is not used in this this case.
         """
-        # Firstly, looks up a token from cache
-        # Since we are looking for token for the current app, NOT for an end user,
-        # notice we give account parameter as None.
-        result = self.app.acquire_token_silent(self.scopes, account=None)
-        if not result:
-            logging.info("No suitable token exists in cache. Let's get a new one from AAD.")
-            result = self.app.acquire_token_for_client(self.scopes)
-
-        if "access_token" in result:
-            logger.debug(
-                "Access token %s... (expires %s)", result["access_token"][:10], result["expires_in"]
-            )
-            return result["access_token"], time.time() + float(result["expires_in"])
-        else:
-            logging.debug(result)
-            msg = "Failed to get Auth from Active Directory:\n{}".format(
-                result["error_description"]
-            )
-            raise RuntimeError(msg)
+        access_token = self.cred.get_token(self.scopes)
+        logger.debug(
+            "Access token %s... (expires %s)", access_token.token[:10], access_token.expires_on
+        )
+        return access_token.token, access_token.expires_on
```

## datarobot/mlops/spooler/memory_spooler.py

```diff
@@ -1,22 +1,13 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import logging
 import sys
-from builtins import object
-from builtins import range
-from builtins import super
 from queue import Empty
 from queue import Full
 from queue import Queue
 
-import datarobot.mlops.install_aliases  # noqa: F401
 from datarobot.mlops.channel.record import Record
 from datarobot.mlops.common import config
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.enums import MLOpsSpoolAction
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.exception import DRSpoolerException
 from datarobot.mlops.spooler.record_spooler import RecordSpooler
@@ -36,15 +27,15 @@
 
 logger = logging.getLogger(__name__)
 
 
 class MemoryRecordSpooler(RecordSpooler):
     __instance = None
 
-    class __InnerMemoryRecordSpooler(object):
+    class __InnerMemoryRecordSpooler:
         DEFAULT_MAX_QUEUE_SIZE = 10 * 1024 * 1024
         DEFAULT_MAX_MESSAGE_SIZE = DEFAULT_MAX_QUEUE_SIZE
         MAX_RECORDS_TO_DEQUEUE = 10
 
         def __init__(self):
             self.queue = None
             self._max_queue_size = self.DEFAULT_MAX_QUEUE_SIZE
@@ -96,45 +87,45 @@
         def enqueue_single_message(self, message_json):
             if not self.initialized:
                 raise DRSpoolerException("Spooler must be opened before using.")
 
             # Check size limit
             message_size = sys.getsizeof(message_json)
             if message_size > self.get_message_byte_size_limit():
-                raise DRSpoolerException("Cannot enqueue record size: {}".format(message_size))
+                raise DRSpoolerException(f"Cannot enqueue record size: {message_size}")
 
             if message_size + self.current_queue_size > self._max_queue_size:
                 raise DRSpoolerException(
                     "Cannot enqueue: memory queue is full: %s", self.current_queue_size
                 )
 
             try:
                 self.queue.put(message_json)
                 self.current_queue_size += message_size
-                logger.debug("Enqueued message of size {}".format(message_size))
+                logger.debug(f"Enqueued message of size {message_size}")
             except Full:
                 raise DRSpoolerException("Failed to publish a message; queue is full.")
 
         def enqueue(self, record_list):
-            logger.debug("About to publish {} messages".format(len(record_list)))
+            logger.debug(f"About to publish {len(record_list)} messages")
 
             for record in record_list:
                 record_json = record.to_json()
                 self.enqueue_single_message(record_json)
-            logger.debug("Published {} messages".format(len(record_list)))
+            logger.debug(f"Published {len(record_list)} messages")
 
         def dequeue_single_message(self):
             if not self.initialized:
                 raise DRSpoolerException("Spooler must be opened before using.")
 
             try:
                 message_json = self.queue.get(block=False)
                 message_size = sys.getsizeof(message_json)
                 self.current_queue_size -= message_size
-                logger.debug("Dequeued message of size {}".format(message_size))
+                logger.debug(f"Dequeued message of size {message_size}")
                 return message_json
             except Empty:
                 return None
 
         def dequeue(self):
             record_list = []
             for _ in range(self.MAX_RECORDS_TO_DEQUEUE):
@@ -146,32 +137,32 @@
 
             return record_list
 
         def get_queue_size(self):
             return self._max_queue_size
 
     def __init__(self):
-        super(MemoryRecordSpooler, self).__init__()
+        super().__init__()
         if MemoryRecordSpooler.__instance is None:
             MemoryRecordSpooler.__instance = MemoryRecordSpooler.__InnerMemoryRecordSpooler()
 
     @staticmethod
     def get_type():
         return SpoolerType.MEMORY
 
     def get_required_config(self):
         return []
 
     def get_optional_config(self):
         return []
 
     def set_config(self):
-        missing = super(MemoryRecordSpooler, self).get_missing_config()
+        missing = super().get_missing_config()
         if len(missing) > 0:
-            raise DRSpoolerException("Configuration values missing: {}".format(missing))
+            raise DRSpoolerException(f"Configuration values missing: {missing}")
         return self.__instance.set_config()
 
     def open(self):
         self.__instance.open()
 
     def close(self):
         self.__instance.close()
```

## datarobot/mlops/spooler/pubsub_spooler.py

```diff
@@ -1,28 +1,21 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import logging
-import sys
 import time
-from builtins import dict
-from builtins import super
+from typing import cast
+
+from google.api_core.exceptions import AlreadyExists
+from google.cloud import pubsub_v1
 
-import datarobot.mlops.install_aliases  # noqa: F401
 from datarobot.mlops.channel.record import Record
 from datarobot.mlops.common import config
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.enums import MLOpsSpoolAction
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.exception import DRSpoolerException
 from datarobot.mlops.spooler.record_spooler import RecordSpooler
-from google.api_core.exceptions import AlreadyExists
-from google.cloud import pubsub_v1
 
 #  Copyright (c) 2020 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
@@ -48,15 +41,15 @@
     PUBSUB_DEFAULT_ACK_DEADLINE_SECONDS = 600  # 10 minutes
 
     PUBSUB_DEFAULT_SLEEP_TIME = 1
     PUBSUB_DEFAULT_MAX_PENDING_MESSAGES = 10
     PUBSUB_DEFAULT_MAX_WAIT_FOR_PENDING_MESSAGES_SECONDS = 180  # 3 minutes
 
     def __init__(self):
-        super(PubSubRecordSpooler, self).__init__()
+        super().__init__()
         self.initialized = False
 
         self._project_id = None
         self._topic_name = None
         self._max_flush_time = None
         self._subscription_name = None
 
@@ -68,39 +61,36 @@
 
         self._last_connect_time = 0
         self._ack_deadline = 0
         self._max_records_dequeue = 0
 
         self._max_wait_time_for_pending_msgs = 0
         self._max_pending_messages = 0
-        if sys.version_info < (3, 0):
-            self._logger.error("PubSub only supports Python3.")
-            raise Exception("PubSub only supports Python 3.")
 
     @staticmethod
     def get_type():
         return SpoolerType.PUBSUB
 
     def get_required_config(self):
         return [ConfigConstants.PUBSUB_PROJECT_ID, ConfigConstants.PUBSUB_TOPIC_NAME]
 
     def get_optional_config(self):
         return [ConfigConstants.PUBSUB_MAX_FLUSH_SECONDS, ConfigConstants.PUBSUB_SUBSCRIPTION_NAME]
 
     def set_config(self):
-        missing = super(PubSubRecordSpooler, self).get_missing_config()
+        missing = super().get_missing_config()
         if len(missing) > 0:
-            raise DRSpoolerException("Configuration values missing: {}".format(missing))
+            raise DRSpoolerException(f"Configuration values missing: {missing}")
 
         data_format_str = config.get_config_default(
             ConfigConstants.SPOOLER_DATA_FORMAT, self.JSON_DATA_FORMAT_STR
         )
         if data_format_str != self.JSON_DATA_FORMAT_STR:
             raise DRSpoolerException(
-                "Data Format: '{}' is not support for the PubSub Spooler".format(data_format_str)
+                f"Data Format: '{data_format_str}' is not support for the PubSub Spooler"
             )
 
         self._max_wait_time_for_pending_msgs = config.get_config_default(
             ConfigConstants.PUBSUB_MAX_WAIT_FOR_PENDING_MESSAGES_SECONDS,
             self.PUBSUB_DEFAULT_MAX_WAIT_FOR_PENDING_MESSAGES_SECONDS,
         )
         self._max_pending_messages = config.get_config_default(
@@ -132,61 +122,59 @@
         # dequeue/subscribe is only needed for testing
         if self._subscription_name is not None:
             self._dequeue_enabled = True
 
             self._full_subscription_name = "projects/{project_id}/subscriptions/{sub}".format(
                 project_id=self._project_id, sub=self._subscription_name
             )
-            self._logger.info(
-                "PubSub full subscription name is {}".format(self._full_subscription_name)
-            )
+            self._logger.info(f"PubSub full subscription name is {self._full_subscription_name}")
         else:
             self._logger.info("No subscription name provided. Dequeue will not be supported.")
 
     def open(self, action=MLOpsSpoolAction.ENQUEUE):
         self.set_config()
 
         # create publisher
         self._publisher = pubsub_v1.PublisherClient()
 
         if self._subscription_name is not None:
             self._subscriber = pubsub_v1.SubscriberClient()
 
         try:
-            self._logger.info("Creating topic {}...".format(self._topic_name))
-            self._publisher.create_topic(self._full_topic_name)
-            self._logger.info("Created topic {}...".format(self._topic_name))
+            self._logger.info(f"Creating topic {self._topic_name}...")
+            self._publisher.create_topic(name=self._full_topic_name)
+            self._logger.info(f"Created topic {self._topic_name}...")
 
         except AlreadyExists:
-            self._logger.info("Topic {} already exists.".format(self._topic_name))
+            self._logger.info(f"Topic {self._topic_name} already exists.")
 
         # dequeue/subscribe is only needed for testing
         if self._dequeue_enabled:
             self._subscriber = pubsub_v1.SubscriberClient()
-            self._logger.info("Creating subscription {}...".format(self._full_subscription_name))
+            self._logger.info(f"Creating subscription {self._full_subscription_name}...")
             try:
                 self._subscriber.create_subscription(
                     name=self._full_subscription_name, topic=self._full_topic_name
                 )
-                self._logger.info("Subscription {} created".format(self._full_subscription_name))
+                self._logger.info(f"Subscription {self._full_subscription_name} created")
 
             except AlreadyExists:
-                self._logger.info(
-                    "Subscription {} already exists.".format(self._full_subscription_name)
-                )
+                self._logger.info(f"Subscription {self._full_subscription_name} already exists.")
 
         self.initialized = True
 
     # used only for testing
     def delete_subscription(self):
-        self._subscriber.delete_subscription(self._full_subscription_name)
+        assert self._subscriber is not None
+        self._subscriber.delete_subscription(subscription=self._full_subscription_name)
 
     # used only for testing
     def delete_topic(self):
-        self._publisher.delete_topic(self._full_topic_name)
+        assert self._publisher is not None
+        self._publisher.delete_topic(topic=self._full_topic_name)
 
     # Used by mlops-cli for dequeue
     def empty(self):
         return self._empty_count >= self.DEFAULT_CONSUMER_MAX_FETCH_BEFORE_SET_EMPTY
 
     def ack_records(self, records_id_list):
         if not self.enable_dequeue_ack_record:
@@ -194,24 +182,25 @@
 
         ack_ids = [
             self._records_pending_ack[record_id]
             for record_id in records_id_list
             if record_id in self._records_pending_ack
         ]
         if len(ack_ids) > 0:
-            self._subscriber.acknowledge(self._full_subscription_name, ack_ids)
+            assert self._subscriber is not None
+            self._subscriber.acknowledge(subscription=self._full_subscription_name, ack_ids=ack_ids)
 
     def close(self):
         flush_time_spent = 0
         wait_interval = 5
         while futures:
-            self._logger.info("Waiting for {} unacked messages.".format(len(futures)))
+            self._logger.info(f"Waiting for {len(futures)} unacked messages.")
             time.sleep(wait_interval)
             flush_time_spent += wait_interval
-            if 0 < self._max_flush_time < wait_interval:
+            if 0 < cast(int, self._max_flush_time) < wait_interval:
                 break
 
     def get_message_byte_size_limit(self):
         return self.PUBSUB_MESSAGE_SIZE_LIMIT_IN_BYTE
 
     def publish_single_record(self, record):
         if not self.initialized:
@@ -222,24 +211,26 @@
 
         record_json = record.to_json()
         record_bytearray = record_json.encode("utf-8")
         record_size = len(record_bytearray)
 
         # Check size limit
         if record_size > self.get_message_byte_size_limit():
-            self._logger.info("Cannot enqueue record of size: {}".format(record_size))
+            self._logger.info(f"Cannot enqueue record of size: {record_size}")
             raise DRSpoolerException(
                 "Record size {} over maximum {}.".format(
                     record_size, self.get_message_byte_size_limit()
                 )
             )
         t0 = time.time()
-        while time.time() < t0 + self._max_wait_time_for_pending_msgs:
-            if len(futures) < self._max_pending_messages:
+        assert self._full_topic_name is not None
+        while time.time() < t0 + cast(int, self._max_wait_time_for_pending_msgs):
+            if len(futures) < cast(int, self._max_pending_messages):
                 futures.update({record_json: None})
+                assert self._publisher is not None
                 f = self._publisher.publish(self._full_topic_name, record_bytearray)
                 futures[record_json] = f
                 f.add_done_callback(_get_callback(f, record_json))
                 return
             else:
                 time.sleep(self.PUBSUB_DEFAULT_SLEEP_TIME)
         else:
@@ -249,65 +240,67 @@
                 "or MLOPS_PUBSUB_MAX_WAIT_FOR_PENDING_MESSAGES_SECONDS (current value is 180 sec.)"
             )
 
     def enqueue(self, record_list):
         if not self.initialized:
             raise DRSpoolerException("Spooler must be opened before using.")
 
-        self._logger.debug("Publishing {} records".format(len(record_list)))
+        self._logger.debug(f"Publishing {len(record_list)} records")
 
         if len(record_list) < 1:
             return
 
         for record in record_list:
             self.publish_single_record(record)
 
-        self._logger.debug("Published {} messages".format(len(record_list)))
+        self._logger.debug(f"Published {len(record_list)} messages")
 
     # dequeue is only provided for testing
     def dequeue(self):
         if self._dequeue_enabled is False:
             raise DRSpoolerException(
                 " You must provide a subscription name on spooler creation to \
             enable dequeue."
             )
 
         if not self.initialized:
             raise DRSpoolerException("Spooler must be opened before using.")
 
         record_list = []
+        assert self._subscriber is not None
         response = self._subscriber.pull(
-            self._full_subscription_name, max_messages=self._max_records_dequeue
+            subscription=self._full_subscription_name,
+            max_messages=cast(int, self._max_records_dequeue),
         )
         ack_ids = []
         for msg in response.received_messages:
             try:
                 msg_bytes = msg.message.data
                 message_json = msg_bytes.decode("utf-8")
                 record = Record.from_json(message_json)
-                self._logger.debug(
-                    "Received message for deployment {}".format(record.get_deployment_id())
-                )
+                self._logger.debug(f"Received message for deployment {record.get_deployment_id()}")
                 record_list.append(record)
 
                 self._add_pending_record(record.get_id(), msg.ack_id)
                 ack_ids.append(msg.ack_id)
             except Exception as e:
-                self._logger.error("Unable to dequeue message: {}".format(e))
+                self._logger.error(f"Unable to dequeue message: {e}")
 
-        self._logger.debug("Received {} messages.".format(len(ack_ids)))
+        self._logger.debug(f"Received {len(ack_ids)} messages.")
         if len(ack_ids) > 0:
             if self.enable_dequeue_ack_record:
                 self._subscriber.modify_ack_deadline(
                     subscription=self._full_subscription_name,
                     ack_ids=ack_ids,
-                    ack_deadline_seconds=self._ack_deadline,
+                    ack_deadline_seconds=cast(int, self._ack_deadline),
                 )
             else:
-                self._subscriber.acknowledge(self._full_subscription_name, ack_ids)
+                self._subscriber.acknowledge(
+                    subscription=self._full_subscription_name, ack_ids=ack_ids
+                )
 
         self._update_empty_count(len(record_list))
 
         return record_list
 
     def __dict__(self):
         return {
@@ -320,10 +313,10 @@
 
 
 def _get_callback(future, data):
     def callback(f):
         try:
             futures.pop(data)
         except Exception:
-            logger.error("Received exception {} for {}.".format(f.exception(), data))
+            logger.error(f"Received exception {f.exception()} for {data}.")
 
     return callback
```

## datarobot/mlops/spooler/rabbitmq_spooler.py

```diff
@@ -1,30 +1,22 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import ssl
 import sys
-from builtins import range
-from builtins import str
-from builtins import super
 from time import sleep
 from time import time
 
-import datarobot.mlops.install_aliases  # noqa: F401
 import pika
+from pika.exceptions import AMQPConnectionError
+
 from datarobot.mlops.channel.record import Record
 from datarobot.mlops.common import config
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.enums import MLOpsSpoolAction
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.exception import DRSpoolerException
 from datarobot.mlops.spooler.record_spooler import RecordSpooler
-from pika.exceptions import AMQPConnectionError
 
 #  Copyright (c) 2020 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
@@ -39,15 +31,15 @@
 
     RABBITMQ_MAX_RECORDS_TO_DEQUEUE = 10
     RABBITMQ_MESSAGE_SIZE_LIMIT_IN_BYTE = 1024 * 1024 * 50
     DEFAULT_PUBLISH_ATTEMPTS = 5
     DEFAULT_PUBLISH_INTERVAL = 0.2  # in seconds
 
     def __init__(self):
-        super(RabbitMQRecordSpooler, self).__init__()
+        super().__init__()
         self.publish_attempts = self.DEFAULT_PUBLISH_ATTEMPTS
         self.publish_interval = self.DEFAULT_PUBLISH_INTERVAL
         self.refresh_conn_time = float("inf")
         self._last_connect_time = 0
         self.initialized = False
 
         self._queue_url = None
@@ -69,24 +61,24 @@
     def get_required_config(self):
         return [ConfigConstants.RABBITMQ_QUEUE_URL, ConfigConstants.RABBITMQ_QUEUE_NAME]
 
     def get_optional_config(self):
         return []
 
     def set_config(self):
-        missing = super(RabbitMQRecordSpooler, self).get_missing_config()
+        missing = super().get_missing_config()
         if len(missing) > 0:
-            raise DRSpoolerException("Configuration values missing: {}".format(missing))
+            raise DRSpoolerException(f"Configuration values missing: {missing}")
 
         data_format_str = config.get_config_default(
             ConfigConstants.SPOOLER_DATA_FORMAT, self.JSON_DATA_FORMAT_STR
         )
         if data_format_str != self.JSON_DATA_FORMAT_STR:
             raise DRSpoolerException(
-                "Data Format: '{}' is not support for the RabbitMQ Spooler".format(data_format_str)
+                f"Data Format: '{data_format_str}' is not support for the RabbitMQ Spooler"
             )
         self._queue_url = config.get_config(ConfigConstants.RABBITMQ_QUEUE_URL)
         self._queue_name = config.get_config(ConfigConstants.RABBITMQ_QUEUE_NAME)
 
         self._ssl_ca_certificate_path = config.get_config_default(
             ConfigConstants.RABBITMQ_SSL_CA_CERTIFICATE_PATH, None
         )
@@ -127,15 +119,15 @@
             self._logger.debug(
                 "Successfully connected to {}, using queue: {}".format(
                     self._queue_url, self._queue_name
                 )
             )
             self.initialized = True
         except AMQPConnectionError as ex:
-            msg = "Fail to establish connection to RabbitMQ: {}".format(ex)
+            msg = f"Fail to establish connection to RabbitMQ: {ex}"
             self._logger.error(msg)
             raise DRSpoolerException(msg)
 
     def close(self):
         if self._channel and not self._channel.is_closed:
             self._channel.close()
 
@@ -181,15 +173,15 @@
                 except pika.exceptions.ProbableAccessDeniedError:
                     # If credentials are invalid, there is no sense to sleep and reconnect
                     raise DRSpoolerException(
                         "Access denied error when publishing a message to RabbitMQ"
                     )
                 except Exception as ex:
                     self._logger.warning(
-                        "Reconnection error when publishing a message to RabbitMQ - {}".format(ex)
+                        f"Reconnection error when publishing a message to RabbitMQ - {ex}"
                     )
                     reason = "reconnection error"
                     continue
 
             try:
                 if self._channel.basic_publish(
                     exchange="",
@@ -204,37 +196,35 @@
                 self._logger.debug("Connection error when publishing a message to RabbitMQ")
                 reason = "connection error"
             except pika.exceptions.ChannelClosed:
                 self._logger.info("Channel closed when publishing a message to RabbitMQ")
                 reason = "channel close"
             except Exception as ex:
                 raise DRSpoolerException(
-                    "Unexpected exception when publishing a message to RabbitMQ - {}".format(ex)
+                    f"Unexpected exception when publishing a message to RabbitMQ - {ex}"
                 )
 
             will_reconnect = True
 
-        raise DRSpoolerException(
-            "Failed to publish a message to RabbitMQ, reason: {}".format(reason)
-        )
+        raise DRSpoolerException(f"Failed to publish a message to RabbitMQ, reason: {reason}")
 
     def enqueue(self, record_list):
-        self._logger.debug("About to publish {} messages".format(len(record_list)))
+        self._logger.debug(f"About to publish {len(record_list)} messages")
 
         for record in record_list:
             record_json = record.to_json()
 
             # Check size limit
             record_size = sys.getsizeof(record_json)
             if record_size > self.get_message_byte_size_limit():
-                raise DRSpoolerException("Cannot enqueue record size: {}".format(record_size))
+                raise DRSpoolerException(f"Cannot enqueue record size: {record_size}")
 
             self.publish_single_message(record_json)
 
-        self._logger.debug("Published {} messages".format(len(record_list)))
+        self._logger.debug(f"Published {len(record_list)} messages")
 
     # Used by mlops-cli for dequeue
     def empty(self):
         q = self._channel.queue_declare(self._queue_name, durable=True)
         q_len = q.method.message_count
         return q_len == 0
```

## datarobot/mlops/spooler/record_spooler.py

```diff
@@ -1,18 +1,11 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import logging
 from abc import ABCMeta
 from abc import abstractmethod
-from builtins import str
 
-import datarobot.mlops.install_aliases  # noqa: F401
 from datarobot.mlops.common import config
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.enums import DataFormat
 from datarobot.mlops.common.enums import MLOpsSpoolAction
 from datarobot.mlops.common.exception import DRCommonException
 
 #  Copyright (c) 2020 DataRobot, Inc. and its affiliates. All rights reserved.
@@ -24,27 +17,25 @@
 #  such source code.
 #
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
 
 
-class RecordSpooler(object):
+class RecordSpooler:
     __metaclass__ = ABCMeta
     BYTES_PER_ROW_OF_DATA = 1024 * 10  # This is a rough estimate
     BYTES_PER_CUSTOM_METRIC_ROW = 100
     JSON_DATA_FORMAT_STR = "JSON"
     BINARY_DATA_FORMAT_STR = "BYTE_ARRAY"
     DEFAULT_DEQUEUE_ACK_RECORDS = True
     DEFAULT_CONSUMER_MAX_FETCH_BEFORE_SET_EMPTY = 5
 
     def __init__(self):
-        self._logger = logging.getLogger(
-            "{}.{}".format(self.__class__.__module__, self.__class__.__name__)
-        )
+        self._logger = logging.getLogger(f"{self.__class__.__module__}.{self.__class__.__name__}")
         self._spool_data_format = DataFormat.JSON
         self.enable_dequeue_ack_record = config.get_config_default(
             ConfigConstants.SPOOLER_DEQUEUE_ACK_RECORDS, self.DEFAULT_DEQUEUE_ACK_RECORDS
         )
         self._records_pending_ack = {}
         self._empty_count = 0
```

## datarobot/mlops/spooler/record_spooler_factory.py

```diff
@@ -1,13 +1,7 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
-import datarobot.mlops.install_aliases  # noqa: F401
 from datarobot.mlops.common import config
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.exception import DRConfigKeyNotFound
 from datarobot.mlops.common.exception import DRUnsupportedType
 from datarobot.mlops.common.singleton import singleton
 from datarobot.mlops.constants import Constants
 from datarobot.mlops.spooler.async_memory_spooler import AsyncMemoryRecordSpooler
@@ -25,15 +19,15 @@
 #
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
 
 
 @singleton
-class RecordSpoolerFactory(object):
+class RecordSpoolerFactory:
     def create(self, spooler_type):
 
         if spooler_type == SpoolerType.STDOUT:
             return StdoutRecordSpooler()
 
         elif spooler_type == SpoolerType.FILESYSTEM:
             return FSRecordSpooler()
@@ -91,15 +85,15 @@
                     "KafkaSpoolChannel failed; missing package; "
                     "need to `pip install {}[kafka]`".format(Constants.OFFICIAL_NAME)
                 )
                 raise RuntimeError(message)
             return KafkaRecordSpooler()
 
         else:
-            raise DRUnsupportedType("Unsupported spooler type: {}".format(spooler_type))
+            raise DRUnsupportedType(f"Unsupported spooler type: {spooler_type}")
 
     def get_current_configured_spooler(self):
         # Try to read the spooler configuration from environment variables
         try:
             spooler_type = SpoolerType.from_name(
                 config.get_config(config.ConfigConstants.SPOOLER_TYPE)
             )
```

## datarobot/mlops/spooler/spooler_offset_manager.py

```diff
@@ -1,18 +1,9 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import logging
 import time
-from builtins import object
-
-import datarobot.mlops.install_aliases  # noqa: F401
-import six
 
 #  Copyright (c) 2020 DataRobot, Inc. and its affiliates. All rights reserved.
 #  Last updated 2023.
 #
 #  DataRobot, Inc. Confidential.
 #  This is unpublished proprietary source code of DataRobot, Inc. and its affiliates.
 #  The copyright notice above does not evidence any actual or intended publication of
@@ -22,21 +13,21 @@
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
 
 
 logger = logging.getLogger(__name__)
 
 
-class FSRecord(object):
+class FSRecord:
     def __init__(self, offset, record):
         self.offset = offset
         self.record = record
 
 
-class OffsetMeta(object):
+class OffsetMeta:
     def __init__(self, partition, offset, record_id):
         self.acknowledge_received = False
         self.retry_counter = 0
         self.timestamp = time.time()
         self.offset = offset
         self.partition = partition
         self.record_id = record_id
@@ -62,15 +53,15 @@
     def is_ack_or_retry_exceed_limit(self, max_retry):
         return self.acknowledge_received or self.retry_counter > max_retry
 
     def is_expired(self, ack_deadline_ms):
         return not self.acknowledge_received and (self.timestamp + ack_deadline_ms) < time.time()
 
 
-class SpoolerOffsetManager(object):
+class SpoolerOffsetManager:
     DEFAULT_PARTITION = 0
 
     def __init__(self, clear_record_timeout_sec, max_retry):
         self.last_clear_records_processed = time.time()
         self.clear_record_timeout_sec = clear_record_timeout_sec
         self.max_retry = max_retry
         self.ack_records = dict()
@@ -82,44 +73,44 @@
         last_committed_offset = offset_meta.get_offset()
 
         if partition not in self.partition_map:
             return
 
         to_remove = [
             offset
-            for offset, _ in six.iteritems(self.partition_map[partition])
+            for offset, _ in self.partition_map[partition].items()
             if offset <= last_committed_offset
         ]
         for key in to_remove:
             del self.partition_map[partition][key]
 
     def clear_records_processed(self):
         if self.last_clear_records_processed + self.clear_record_timeout_sec < time.time():
             self.records_processed.clear()
             self.last_clear_records_processed = time.time()
 
     def find_next_offsets(self):
         offset_map = dict()
-        for partition, ack_record_map in six.iteritems(self.partition_map):
+        for partition, ack_record_map in self.partition_map.items():
             offset_list = [
                 offset_meta
-                for _, offset_meta in six.iteritems(ack_record_map)
+                for _, offset_meta in ack_record_map.items()
                 if offset_meta.is_ack_or_retry_exceed_limit(self.max_retry)
             ]
             offset_map[partition] = offset_list
         return offset_map
 
     def find_next_offset_single_partition(self):
         return self.find_next_offsets()[self.DEFAULT_PARTITION]
 
     def find_next_expired_offset(self, ack_deadline_ms, partition=DEFAULT_PARTITION):
         if partition not in self.partition_map:
             return
 
-        for _, offset_meta in six.iteritems(self.partition_map[partition]):
+        for _, offset_meta in self.partition_map[partition].items():
             if offset_meta.is_expired(ack_deadline_ms):
                 return offset_meta
         return None
 
     def ack_record(self, offset, partition=DEFAULT_PARTITION):
         if partition not in self.partition_map:
             return
```

## datarobot/mlops/spooler/sqs_spool.py

```diff
@@ -1,19 +1,11 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
 import sys
-from builtins import int
-from builtins import str
-from builtins import super
 
 import boto3
-import datarobot.mlops.install_aliases  # noqa: F401
+
 from datarobot.mlops.channel.record import Record
 from datarobot.mlops.common import config
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.config import ConfigKeys
 from datarobot.mlops.common.enums import MLOpsSpoolAction
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.enums import SQSQueueType
@@ -41,15 +33,15 @@
     # Each SQS message cannot be greater than 256KB.
     # The prediction data will be limited in 256000 bytes to leave some room for metadata
     SQS_MAX_MESSAGE_SIZE = 256000
     SQS_DEFAULT_MESSAGE_GROUP = "AAAAAAAA"
     SQS_DEFAULT_MAX_NUMBER_OF_MESSAGES = 10  # max allowed by SQS batch
 
     def __init__(self, sqs_client=None):
-        super(SQSRecordSpooler, self).__init__()
+        super().__init__()
         self.initialized = False
 
         self._queue_name = None
         self._queue_url = None
         self._message_group_id = None
 
         self._max_number_of_messages = 0
@@ -79,23 +71,23 @@
                 "{} is not set. Using {}.".format(
                     ConfigConstants.SQS_QUEUE_NAME.name, ConfigConstants.SQS_QUEUE_URL.name
                 )
             )
             self._queue_url = config.get_config_default(ConfigConstants.SQS_QUEUE_URL, None)
             if self._queue_url is None:
                 raise DRSpoolerException(
-                    "Configuration values missing: {}".format(ConfigConstants.SQS_QUEUE_NAME.name)
+                    f"Configuration values missing: {ConfigConstants.SQS_QUEUE_NAME.name}"
                 )
 
         data_format_str = config.get_config_default(
             ConfigConstants.SPOOLER_DATA_FORMAT, self.JSON_DATA_FORMAT_STR
         )
         if data_format_str != self.JSON_DATA_FORMAT_STR:
             raise DRSpoolerException(
-                "Data Format: '{}' is not supported for the SQS Spooler".format(data_format_str)
+                f"Data Format: '{data_format_str}' is not supported for the SQS Spooler"
             )
 
         self._max_number_of_messages = config.get_config_default(
             config.ConfigConstants.SPOOLER_DEQUEUE_MAX_RECORDS_PER_CALL,
             self.SQS_DEFAULT_MAX_NUMBER_OF_MESSAGES,
         )
         # Use deployment id for FIFO queue message group id
@@ -105,17 +97,15 @@
 
     def open(self, action=MLOpsSpoolAction.ENQUEUE):
         self.set_config()
         if self._sqs_client is None:
             try:
                 self._sqs_client = boto3.client("sqs")
             except Exception as e:
-                raise DRApiException(
-                    "Failed to initialize AWS SQS Client with error: {}".format(str(e))
-                )
+                raise DRApiException(f"Failed to initialize AWS SQS Client with error: {str(e)}")
 
         if self._queue_name is not None:
             response = self._sqs_client.get_queue_url(QueueName=self._queue_name)
             self._queue_url = response["QueueUrl"]
 
         self._validate_url(self._queue_url)
 
@@ -133,15 +123,15 @@
         num_messages = len(entries)
         try:
             response = self._sqs_client.send_message_batch(
                 QueueUrl=self._queue_url, Entries=entries
             )
             if response["ResponseMetadata"]["HTTPStatusCode"] != 200:
                 raise DRSpoolerException(
-                    "Failed to send message to AWS SQS queue - {}".format(self._queue_url)
+                    f"Failed to send message to AWS SQS queue - {self._queue_url}"
                 )
             return num_messages
         except Exception as e:
             raise DRSpoolerException(
                 "Failed to send message to AWS SQS queue - {} with error: {}".format(
                     self._queue_url, str(e)
                 )
@@ -154,15 +144,15 @@
         if not self.initialized:
             raise DRSpoolerException("Spooler must be opened before using.")
 
         records_send = 0
         entries = []
         record_list_size = 0
 
-        self._logger.debug("About to publish {} messages".format(len(record_list)))
+        self._logger.debug(f"About to publish {len(record_list)} messages")
 
         for idx, record in enumerate(record_list, start=1):
             record_json = record.to_json()
             entry = {
                 "Id": str(idx),
                 "MessageBody": record_json,
             }
@@ -182,27 +172,27 @@
                 )
             else:
                 # Send messages once reach max message size or exceed limit
                 if (
                     len(entries) == self._max_number_of_messages
                     or record_list_size + record_size >= self._message_byte_size_limit
                 ):
-                    self._logger.debug("SQS Overflow at {} messages".format(len(entries)))
+                    self._logger.debug(f"SQS Overflow at {len(entries)} messages")
                     records_send += self._send_sqs_batch(entries)
-                    self._logger.debug("Sent {} messages".format(len(entries)))
+                    self._logger.debug(f"Sent {len(entries)} messages")
                     del entries[:]
                     record_list_size = 0
 
                 # Add entry to list
                 entries.append(entry)
                 record_list_size += record_size
 
         if len(entries) > 0:
             records_send += self._send_sqs_batch(entries)
-            self._logger.debug("Sent {} messages".format(len(entries)))
+            self._logger.debug(f"Sent {len(entries)} messages")
 
     def ack_records(self, record_id_list):
         if not self.enable_dequeue_ack_record or record_id_list is None:
             return
 
         entries = []
         for record_id in record_id_list:
@@ -225,15 +215,15 @@
             raise DRSpoolerException(
                 "Failed to receive message from AWS SQS queue - {} "
                 "with error: {}".format(self._queue_url, str(e))
             )
 
         if response["ResponseMetadata"]["HTTPStatusCode"] != 200:
             raise DRSpoolerException(
-                "Fail to receive message from AWS SQS queue - {}".format(self._queue_url)
+                f"Fail to receive message from AWS SQS queue - {self._queue_url}"
             )
 
         record_list = list()
         # If there is no message, the http status code is still 200. Need to check the response
         if "Messages" not in response:
             return record_list
```

## datarobot/mlops/spooler/stdout_spooler.py

```diff
@@ -1,15 +1,7 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-
-from builtins import super
-
-import datarobot.mlops.install_aliases  # noqa: F401
 from datarobot.mlops.common import config
 from datarobot.mlops.common.config import ConfigConstants
 from datarobot.mlops.common.enums import MLOpsSpoolAction
 from datarobot.mlops.common.enums import SpoolerType
 from datarobot.mlops.common.exception import DRSpoolerException
 from datarobot.mlops.spooler.record_spooler import RecordSpooler
 
@@ -24,15 +16,15 @@
 #  This file and its contents are subject to DataRobot Tool and Utility Agreement.
 #  For details, see
 #  https://www.datarobot.com/wp-content/uploads/2021/07/DataRobot-Tool-and-Utility-Agreement.pdf.
 
 
 class StdoutRecordSpooler(RecordSpooler):
     def __init__(self):
-        super(StdoutRecordSpooler, self).__init__()
+        super().__init__()
         self.initialized = False
 
     @staticmethod
     def get_type():
         return SpoolerType.STDOUT
 
     def get_required_config(self):
@@ -41,21 +33,21 @@
     def get_optional_config(self):
         return []
 
     def get_message_byte_size_limit(self):
         return -1
 
     def set_config(self):
-        missing = super(StdoutRecordSpooler, self).get_missing_config()
+        missing = super().get_missing_config()
         if len(missing) > 0:
-            raise DRSpoolerException("Configuration values missing: {}".format(missing))
+            raise DRSpoolerException(f"Configuration values missing: {missing}")
         data_format_str = config.get_config_default(ConfigConstants.SPOOLER_DATA_FORMAT, None)
         if data_format_str:
             raise DRSpoolerException(
-                "Data Format: '{}' is not supported for the Stdout Spooler".format(data_format_str)
+                f"Data Format: '{data_format_str}' is not supported for the Stdout Spooler"
             )
 
     def open(self, action=MLOpsSpoolAction.ENQUEUE):
         self.set_config()
         self.initialized = True
 
     def enqueue(self, record_list):
```

## Comparing `datarobot_mlops-9.0.7.data/data/share/mlops/agent.yaml` & `datarobot_mlops-9.1.1b1.data/data/share/mlops/agent.yaml`

 * *Files identical despite different names*

## Comparing `datarobot_mlops-9.0.7.data/data/share/mlops/mlops.log4j2.properties` & `datarobot_mlops-9.1.1b1.data/data/share/mlops/mlops.log4j2.properties`

 * *Files identical despite different names*

## Comparing `datarobot_mlops-9.0.7.dist-info/METADATA` & `datarobot_mlops-9.1.1b1.dist-info/METADATA`

 * *Files 14% similar despite different names*

```diff
@@ -1,47 +1,44 @@
 Metadata-Version: 2.1
 Name: datarobot-mlops
-Version: 9.0.7
+Version: 9.1.1b1
 Summary: datarobot-mlops library to read and report MLOps statistics
 Home-page: http://datarobot.com
 Author: DataRobot
 Author-email: support@datarobot.com
 Maintainer: DataRobot
 Maintainer-email: info@datarobot.com
 License: DataRobot Tool and Utility Agreement
 Project-URL: Documentation, https://docs.datarobot.com/en/docs/mlops/deployment/mlops-agent/spooler.html#mlops-library-configuration
-Classifier: Programming Language :: Python :: 2
-Classifier: Programming Language :: Python :: 2.7
+Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: License :: Other/Proprietary License
 Classifier: Operating System :: MacOS
 Classifier: Operating System :: POSIX
 Classifier: Operating System :: Unix
-Requires-Python: >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, !=3.5.*, !=3.6.*
+Requires-Python: >=3.7
 Description-Content-Type: text/markdown
-Requires-Dist: future (>=0.16)
 Requires-Dist: python-dateutil
 Requires-Dist: pandas
 Requires-Dist: pyyaml
 Requires-Dist: py4j (<1,>=0.10.9.1)
-Requires-Dist: datarobot ; python_version < "3"
 Requires-Dist: enum34 ; python_version < "3.4"
 Requires-Dist: orjson (>=3) ; python_version >= "3.6"
 Provides-Extra: aggregator
 Requires-Dist: datarobot-mlops-stats-aggregator (>=8.2.4) ; extra == 'aggregator'
 Provides-Extra: aws
 Requires-Dist: boto3 (<2,>=1.11.4) ; extra == 'aws'
 Provides-Extra: azure
-Requires-Dist: msal (<2,>=0.6.1) ; extra == 'azure'
+Requires-Dist: azure-identity (>=1.0) ; extra == 'azure'
 Provides-Extra: google
-Requires-Dist: google-cloud-pubsub (<2,>=1.7) ; extra == 'google'
+Requires-Dist: google-cloud-pubsub (<3,>=2) ; extra == 'google'
 Provides-Extra: kafka
 Requires-Dist: confluent-kafka (<2,>=1.5.0) ; extra == 'kafka'
 Requires-Dist: certifi ; extra == 'kafka'
 Provides-Extra: rabbitmq
 Requires-Dist: pika (<1,>=0.13.1) ; extra == 'rabbitmq'
 
 # DataRobot MLOps metrics reporting library
@@ -69,15 +66,23 @@
 * `kafka` - Installs dependencies for the `kafka` spooler type.
 * `azure` - Installs dependencies that enable Azure Active Directory authentication for the `kafka` spooler type.
 * `aws` - Installs dependencies for the `sqs` spooler type.
 
 You can install these extra dependencies using `pip`, for example:
 
 ```shell
-pip install datarobot-mlops[rabbitmq]
+pip install 'datarobot-mlops[rabbitmq]'
 ```
 
 You can also install multiple sets of dependencies at once, for example:
 
 ```shell
-pip install datarobot-mlops[kafka,azure]
+pip install 'datarobot-mlops[kafka,azure]'
 ```
+
+### Supported Python Versions
+
+Python >= 3.7
+
+The last version of this library compatible with Python 2.7 is `datarobot-mlops~=9.0`.
+
+
```

## Comparing `datarobot_mlops-9.0.7.dist-info/RECORD` & `datarobot_mlops-9.1.1b1.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,38 +1,38 @@
 datarobot/mlops/__init__.py,sha256=AKjMKTNwSwhUBVbzAe95rrPT428dkII5aszufDWlEYw,597
-datarobot/mlops/agent.py,sha256=pCeJ7zGbUzuTiZGHpp5klQ4je6pWIgp9flxCPpZD_PU,21600
-datarobot/mlops/constants.py,sha256=1sBHHWcGnEmBudO_vCgyK_L4_N2ogAy7dXxyKxRWAII,1174
-datarobot/mlops/event.py,sha256=4HBVQYZ2qBX6H48KqCHZCtakhDKD8bE-jlpa5MLvNPY,3933
-datarobot/mlops/install_aliases.py,sha256=rqp8S7CkaaHdYM9UDjD1cRHrGFKKJZL3Ni-Ke1ND3hw,754
-datarobot/mlops/json_shim.py,sha256=UKmncvg9UFVfPfScBwD7VWw2hFGw3DrjIg3xjcWVEo4,3301
-datarobot/mlops/metric.py,sha256=UIoa6NdyofQLK0n5SaUt_WRhXFAMG6hjraL5r2sOs3I,31394
-datarobot/mlops/mlops.py,sha256=1LsdYaOtJgRlDzIniJfr4PLxBdp_KJqFXMulH1fNKLU,39699
-datarobot/mlops/model.py,sha256=N6t_n5-Shlc4rnb4nUVsni7L6KTD4jZeDV3HSwL0dlY,36876
+datarobot/mlops/agent.py,sha256=zbt31wzbURHLQMmMXkQTPIVNGiiitIYsJRptzS4EGrA,21276
+datarobot/mlops/constants.py,sha256=fTAcqXJLKcSGMhadAc5DpZbdfGKGEC2pMXN5RkmL878,1023
+datarobot/mlops/event.py,sha256=3XYENd0P8qa1itpjAfyqvmZnN6uSaVAo1ZnxVE4SeKw,4241
+datarobot/mlops/json_shim.py,sha256=ju_DRuaaGLOqmPBJTMCJHoLkHbf_qXhsQo5dUyLHdGk,3068
+datarobot/mlops/metric.py,sha256=EestF5vM360DFJA1kkOGAPl6BSTV8YpD0rgAP0m-I1Y,30722
+datarobot/mlops/mlops.py,sha256=E04HBkaBDB2gbWKZr679EFs1yrwDaNVse8CZNiUpdxI,39436
+datarobot/mlops/model.py,sha256=uTPyonl_QD7jCD_Xmt7PXE9VNyTsmO1WFVzTrop19mE,43371
 datarobot/mlops/channel/__init__.py,sha256=tjNa8K5lXmMIeAgvdxrcwqKXqunstSuvbqL0xklSess,532
-datarobot/mlops/channel/output_channel_queue.py,sha256=-MG66zKczJMwhrqmAFguVmMqBjPhqCzKQL_8XZpmVc0,19456
-datarobot/mlops/channel/record.py,sha256=gaYpJQ_nFGBziEl4eiel98dqt0cVGIWTB7A8_zGJbJs,13720
+datarobot/mlops/channel/output_channel_queue.py,sha256=obtmtv45toCKVccqO6TrY8v6dtHFmsxx4wmiNgj54OI,19171
+datarobot/mlops/channel/record.py,sha256=MGIqPnXxdnVOe9rmWODJzzkVlPEW2CxkEgl9S-WJssg,13375
 datarobot/mlops/common/__init__.py,sha256=tjNa8K5lXmMIeAgvdxrcwqKXqunstSuvbqL0xklSess,532
-datarobot/mlops/common/aggregation_util.py,sha256=SEitaPq-vnTCXSxQ7704kwiBLPoER3sXeGJY41FbOo4,8379
-datarobot/mlops/common/config.py,sha256=LuOaCjViD1RZh-UMnRcQOBoKhHOvFIrS3accXbhbTts,17494
-datarobot/mlops/common/enums.py,sha256=QONaz4K8TrNiToLJXNoXTsYEA6WxOv2sHON0sH-2c5o,2679
-datarobot/mlops/common/exception.py,sha256=OP_lpgIx02hqAtejCtfEUT-81Wmo2Pe8rKJKS54sRU8,1285
-datarobot/mlops/common/prediction_util.py,sha256=BZk5phgTAQzURhX9HA4SSKjXpfCkYz5e8PWTe1FVGzY,2411
-datarobot/mlops/common/singleton.py,sha256=a5-EClOUwmuyV7ihT9hzy3veqIv30BnqHfqLBv4sot8,1065
-datarobot/mlops/common/stringutil.py,sha256=RiXvSCo7p6npHE_-0dOZZlNnPO5vbxL_T556r5F_jUk,1123
+datarobot/mlops/common/aggregation_util.py,sha256=efVhOrgQV0ILq55_xs7ve0kHifQ7JhyHc0oh4YErGOI,8149
+datarobot/mlops/common/config.py,sha256=8QQ6GU96A3EMWRWQv7ctq7CDq4AiOudm2WZK0DoXa4c,16970
+datarobot/mlops/common/enums.py,sha256=oqMURq6mYtBVlOet9fkCLdSuYcqSfZnbZyp5HhwECvU,2409
+datarobot/mlops/common/exception.py,sha256=6eJCd4mI1IxZpTWPSFmLqXKcmIHiNXXQu2PMXr-KdKM,1081
+datarobot/mlops/common/prediction_util.py,sha256=j8u8HyTN4yqdAJDdBaXeJvszP3D95mxPCIZE_6T_NvA,2182
+datarobot/mlops/common/singleton.py,sha256=d3Vx_rUgnWh-8qpJ6PtCBxQfSd46yjPwOQi7TUuzD2k,861
+datarobot/mlops/common/stringutil.py,sha256=mzQ3fN3-pWiQR5BCf-VF3GrqbpmXDZuRUhX_ZcJk4as,919
+datarobot/mlops/common/version_util.py,sha256=p4bPqjtPVdSzJYavXoT9nGMVt-RqR3RGIEwbhWyBVYw,3872
 datarobot/mlops/spooler/__init__.py,sha256=tjNa8K5lXmMIeAgvdxrcwqKXqunstSuvbqL0xklSess,532
-datarobot/mlops/spooler/async_memory_spooler.py,sha256=t7Bib8TMv-TUyBzvE19Vo264D9VKmvuyhlQlpXjGeUw,7626
-datarobot/mlops/spooler/filesystem_spooler.py,sha256=FWXQ3wW7Mgd2USXjzNlzA-SrnpDeYsaHLG5eg5holDM,32861
-datarobot/mlops/spooler/kafka_spooler.py,sha256=pbuwoY5jbRC3nziKFtzHJxZmwmto2dfWjc6oMl5ndkU,22445
-datarobot/mlops/spooler/memory_spooler.py,sha256=udlOf1cqafuxm76iVbtbC_z0AB4FmmtWJDTWla3asec,7363
-datarobot/mlops/spooler/pubsub_spooler.py,sha256=QkyQWn0Mj5AlScQEAMRdjdFz8ooh5YtARCDJ-RxLU0E,12670
-datarobot/mlops/spooler/rabbitmq_spooler.py,sha256=S_69UNC9hisx4kIqxuxTsd4--2gYhBJWYuDOUeDcQk8,10529
-datarobot/mlops/spooler/record_spooler.py,sha256=soiC_f1CgBRVNHwvOYy7xhUn2fiv125P6tzP2UwfFwM,4681
-datarobot/mlops/spooler/record_spooler_factory.py,sha256=anDzuBV3wXWH6TGmb0wIyHwilDSX5IomCBDqiMiT6G4,4685
-datarobot/mlops/spooler/spooler_offset_manager.py,sha256=PKsq1xSpmdy0tn7MZjFlFm1_eGC2LBAhf34f5JAgBcA,4749
-datarobot/mlops/spooler/sqs_spool.py,sha256=pkYyoJYbT5pZ4O_jvddjtsOZi3__oDOuRMgrw8JS-BY,10780
-datarobot/mlops/spooler/stdout_spooler.py,sha256=kTDCrXviAav6Y5VPhR-OC-XvGxwQocvDXTicQXkLqwY,2509
-datarobot_mlops-9.0.7.data/data/share/mlops/agent.yaml,sha256=gCkDlb3EA1bzfE6Bf5S8XMBA10xZ4OoSXWDJeDXzDrw,2570
-datarobot_mlops-9.0.7.data/data/share/mlops/mlops.log4j2.properties,sha256=HEofKe-I_176Wd4r30Z9BEnQtkHbxOek-4Xzim54gaQ,1395
-datarobot_mlops-9.0.7.dist-info/METADATA,sha256=zKFskhVitsf8Vv6h_y8lc5zzBUbCXS-GThU6LYggERI,3184
-datarobot_mlops-9.0.7.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
-datarobot_mlops-9.0.7.dist-info/top_level.txt,sha256=RTK5ZHErXe7mZUlXWQRjQpqFdWw3qmpF95Lz7ViL2Lc,10
-datarobot_mlops-9.0.7.dist-info/RECORD,,
+datarobot/mlops/spooler/async_memory_spooler.py,sha256=LXv0tJBC6yqJg1qcvtVH3RjFplP-XQH2zQ_MukrvwMg,7227
+datarobot/mlops/spooler/filesystem_spooler.py,sha256=WKXMeZK_r99CV246xu05kVa2GgYNewSyzSMunUG3FiI,34711
+datarobot/mlops/spooler/kafka_spooler.py,sha256=2eZFCEQVOvBLtbMuUM2BEWZakkcEA2RukjloW0IcgFk,21987
+datarobot/mlops/spooler/memory_spooler.py,sha256=lEzDbG6kqPzdzLW-p2WOzTWHRB1DQRrXygw2NU3Qq4s,6972
+datarobot/mlops/spooler/pubsub_spooler.py,sha256=I5g7EEjdXXERMU2GOusLOg1-WGgWTsZ7yL86fxoO6g4,12434
+datarobot/mlops/spooler/rabbitmq_spooler.py,sha256=ptyPQNQSnuICUxZrAJrg-vIYUvfoXXdSHzTN2Shk2sw,10100
+datarobot/mlops/spooler/record_spooler.py,sha256=i4HZnwHwQnHpCYZ_zyHUg9bFUbUonS619K8TdLPWwu0,4413
+datarobot/mlops/spooler/record_spooler_factory.py,sha256=9QG-uoqVM02ZOmVKn4xqy4tNNfr2oEWEFm1XzpsHbJw,4466
+datarobot/mlops/spooler/spooler_offset_manager.py,sha256=ty-a0bCQ7fd7wF680e1MSNjc9rX2fJzH4Si7FBeLFXM,4454
+datarobot/mlops/spooler/sqs_spool.py,sha256=UgLz98R7T7hRWnzjYB2PqhDvvqkCCvFKCbGs1PYGR2k,10369
+datarobot/mlops/spooler/stdout_spooler.py,sha256=JugaGLxvTzAiCZTbs5ee8i--9X1yXxAT2jZ6G-d5Vcw,2212
+datarobot_mlops-9.1.1b1.data/data/share/mlops/agent.yaml,sha256=gCkDlb3EA1bzfE6Bf5S8XMBA10xZ4OoSXWDJeDXzDrw,2570
+datarobot_mlops-9.1.1b1.data/data/share/mlops/mlops.log4j2.properties,sha256=HEofKe-I_176Wd4r30Z9BEnQtkHbxOek-4Xzim54gaQ,1395
+datarobot_mlops-9.1.1b1.dist-info/METADATA,sha256=BRrbdCwBhAV08HDuzxodaHW5GSrEz7XpTe-8JZ5cj_c,3107
+datarobot_mlops-9.1.1b1.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+datarobot_mlops-9.1.1b1.dist-info/top_level.txt,sha256=RTK5ZHErXe7mZUlXWQRjQpqFdWw3qmpF95Lz7ViL2Lc,10
+datarobot_mlops-9.1.1b1.dist-info/RECORD,,
```

