# Comparing `tmp/synapgrad-0.1.1-py3-none-any.whl.zip` & `tmp/synapgrad-0.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,21 +1,21 @@
-Zip file size: 18033 bytes, number of entries: 19
--rw-rw-rw-  2.0 fat       58 b- defN 23-Apr-07 12:34 synapgrad/__init__.py
--rw-rw-rw-  2.0 fat    17246 b- defN 23-Apr-08 02:33 synapgrad/engine.py
--rw-rw-rw-  2.0 fat      173 b- defN 23-Apr-07 10:49 synapgrad/nn/__init__.py
--rw-rw-rw-  2.0 fat     1643 b- defN 23-Apr-07 20:58 synapgrad/nn/activations.py
+Zip file size: 18906 bytes, number of entries: 19
+-rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-08 18:28 synapgrad/__init__.py
+-rw-rw-rw-  2.0 fat    19226 b- defN 23-Apr-08 19:15 synapgrad/engine.py
+-rw-rw-rw-  2.0 fat      200 b- defN 23-Apr-08 15:28 synapgrad/nn/__init__.py
+-rw-rw-rw-  2.0 fat     1577 b- defN 23-Apr-08 16:01 synapgrad/nn/activations.py
 -rw-rw-rw-  2.0 fat     1666 b- defN 23-Apr-08 00:44 synapgrad/nn/layers.py
--rw-rw-rw-  2.0 fat     3935 b- defN 23-Apr-07 22:01 synapgrad/nn/losses.py
+-rw-rw-rw-  2.0 fat     2867 b- defN 23-Apr-08 16:17 synapgrad/nn/losses.py
 -rw-rw-rw-  2.0 fat     2413 b- defN 23-Apr-07 23:47 synapgrad/nn/modules.py
 -rw-rw-rw-  2.0 fat     2131 b- defN 23-Apr-08 00:26 synapgrad/nn/neurons.py
--rw-rw-rw-  2.0 fat       46 b- defN 23-Apr-07 09:15 synapgrad/optim/__init__.py
--rw-rw-rw-  2.0 fat      746 b- defN 23-Apr-07 20:59 synapgrad/optim/optimizers.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-07 10:48 synapgrad/utils/__init__.py
+-rw-rw-rw-  2.0 fat       46 b- defN 23-Apr-08 14:30 synapgrad/optim/__init__.py
+-rw-rw-rw-  2.0 fat     2226 b- defN 23-Apr-08 14:30 synapgrad/optim/optimizers.py
+-rw-rw-rw-  2.0 fat      134 b- defN 23-Apr-08 18:50 synapgrad/utils/__init__.py
 -rw-rw-rw-  2.0 fat     2965 b- defN 23-Apr-07 09:47 synapgrad/utils/data.py
--rw-rw-rw-  2.0 fat     1007 b- defN 23-Apr-07 11:26 synapgrad/utils/graph.py
+-rw-rw-rw-  2.0 fat     1173 b- defN 23-Apr-08 19:37 synapgrad/utils/graph.py
 -rw-rw-rw-  2.0 fat    10050 b- defN 23-Apr-07 12:39 synapgrad/utils/train.py
--rw-rw-rw-  2.0 fat     1088 b- defN 23-Apr-08 03:50 synapgrad-0.1.1.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     3895 b- defN 23-Apr-08 03:50 synapgrad-0.1.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-08 03:50 synapgrad-0.1.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       10 b- defN 23-Apr-08 03:50 synapgrad-0.1.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     1522 b- defN 23-Apr-08 03:50 synapgrad-0.1.1.dist-info/RECORD
-19 files, 50686 bytes uncompressed, 15559 bytes compressed:  69.3%
+-rw-rw-rw-  2.0 fat     1088 b- defN 23-Apr-08 19:56 synapgrad-0.2.0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     4433 b- defN 23-Apr-08 19:56 synapgrad-0.2.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-08 19:56 synapgrad-0.2.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       10 b- defN 23-Apr-08 19:56 synapgrad-0.2.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     1525 b- defN 23-Apr-08 19:56 synapgrad-0.2.0.dist-info/RECORD
+19 files, 53894 bytes uncompressed, 16432 bytes compressed:  69.5%
```

## zipnote {}

```diff
@@ -36,23 +36,23 @@
 
 Filename: synapgrad/utils/graph.py
 Comment: 
 
 Filename: synapgrad/utils/train.py
 Comment: 
 
-Filename: synapgrad-0.1.1.dist-info/LICENSE
+Filename: synapgrad-0.2.0.dist-info/LICENSE
 Comment: 
 
-Filename: synapgrad-0.1.1.dist-info/METADATA
+Filename: synapgrad-0.2.0.dist-info/METADATA
 Comment: 
 
-Filename: synapgrad-0.1.1.dist-info/WHEEL
+Filename: synapgrad-0.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: synapgrad-0.1.1.dist-info/top_level.txt
+Filename: synapgrad-0.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: synapgrad-0.1.1.dist-info/RECORD
+Filename: synapgrad-0.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## synapgrad/__init__.py

```diff
@@ -1,2 +1,2 @@
 
-from .engine import Tensor, tensor, no_grad, manual_seed
+from .engine import Tensor, tensor, no_grad, manual_seed, retain_grads
```

## synapgrad/engine.py

```diff
@@ -1,14 +1,14 @@
 import random
 import numpy as np
 from typing import Iterable, Union
 
 
 gradient__ = True
-
+retain_grads__ = False
 
 class no_grad:
     
     def __init__(self) -> None:
         self.prev = gradient__
     
     def __enter__(self):
@@ -16,14 +16,27 @@
         gradient__ = False
         
     def __exit__(self, exc_type, exc_val, exc_tb):
         global gradient__
         gradient__ = self.prev
         
 
+class retain_grads:
+    def __init__(self) -> None:
+        self.prev = retain_grads__
+    
+    def __enter__(self):
+        global retain_grads__
+        retain_grads__ = True
+        
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        global retain_grads__
+        retain_grads__ = self.prev
+        
+
 def manual_seed(seed:int):
     np.random.seed(seed)
     random.seed(seed)
 
 
 def tensor(data, requires_grad=False) -> 'Tensor':
     return Tensor(data, requires_grad=requires_grad)
@@ -55,20 +68,38 @@
         would be shape (2,3) and the gradient of v1 is (3,), so a sum() operation over axis 0 is required.
         This function calculates the necessary axes in which to perform the sum() operation.
 
         Args:
             tensor (Tensor): Tensor whose gradient is going to be updated
             grad (np.ndarray): Gradient array to add
         """
-        tensor_shape = len(tensor.data.shape); grad_shape = len(grad.data.shape)
-        diff_axis = grad_shape-tensor_shape
+        def get_incompatible_dims(tensor_shape, grad_shape) -> tuple:
+            not_compatible_dims = []
+            for i, (tdim, gdim) in enumerate(zip(tensor_shape[::-1], grad_shape[::-1])):
+                print(tdim, gdim)
+                if gdim != 1 and tdim != gdim:
+                    not_compatible_dims.append(len(grad_shape)-1-i)
+            not_compatible_dims = tuple(not_compatible_dims)
+            return not_compatible_dims
+            
+        
+        if len(tensor.data.squeeze().shape) == 0:
+            tensor._grad += grad.sum()
+            return
+        
+        tensor_shape = tensor.data.shape; grad_shape = grad.data.shape
+        diff_axis = len(grad_shape)-len(tensor_shape)
         sum_axis = None if diff_axis <= 0 else tuple(np.arange(abs(diff_axis), dtype=np.int8).tolist())
-              
+        
         if tensor.grad.matches_shape(grad) or sum_axis is None: 
-            tensor._grad += grad
+            if sum_axis is None and not tensor.grad.matches_shape(grad):
+                not_compatible_dims = get_incompatible_dims(tensor_shape, grad_shape)
+                tensor += grad.sum(axis=not_compatible_dims)
+            else:
+                tensor._grad += grad
         else:
             tensor._grad += grad.sum(axis=sum_axis)
     
     
     def __add__(self, summand:'Tensor') -> 'Tensor':
         summand = summand if isinstance(summand, Tensor) else Tensor(summand)
         r_grad = self.requires_grad or summand.requires_grad
@@ -298,24 +329,51 @@
         return out
     
     
     def transpose(self, dim0:int, dim1:int) -> 'Tensor':
         """ Transpose tensor, dim0 and dim1 are swapped (0, 1 for 2D tensor)"""
         out = Tensor(self.data.swapaxes(dim0, dim1), (self,), '<Transpose>', requires_grad=self.requires_grad)
         
-        
         def _backward():
             if self.requires_grad:
                 self._grad += out._grad.swapaxes(dim0, dim1)
                 
         out._backward = _backward
         
         return out
     
     
+    def exp(self) -> 'Tensor':
+        return np.e**self
+    
+    
+    def log(self) -> 'Tensor':
+        out = Tensor(np.log(self.data), (self,), '<Log>', requires_grad=self.requires_grad)
+        
+        def _backward():
+            if self.requires_grad:
+                self._grad += (out._grad / (self.data + 1e-10)) 
+                
+        out._backward = _backward
+        
+        return out
+    
+    
+    def sqrt(self) -> 'Tensor':
+        out = Tensor(np.sqrt(self.data), (self,), '<Sqrt>', requires_grad=self.requires_grad)
+        
+        def _backward():
+            if self.requires_grad:
+                self._grad += out._grad / (2 * out.data)
+                
+        out._backward = _backward
+        
+        return out
+    
+    
     def backward(self, grad:np.ndarray=None):
         if not self.requires_grad:
             raise RuntimeError("Trying to call backward on Tensor with requires_grad=False")
 
         if grad is None:
             if self.data.size > 1:
                 raise RuntimeError("grad must be specified for non-scalar tensors")
@@ -342,15 +400,15 @@
         visit_node(self)
 
         # Go one tensor at a time and apply the chain rule to get its gradient
         self._grad = grad
         for i, node in enumerate(reversed(ordered_nodes)):
             if node.grad_fn is not None:
                 node.grad_fn()
-            if node is not self and not node.is_leaf and not node._retain_grad:
+            if node is not self and not node.is_leaf and not node._retain_grad and not retain_grads__:
                 del node._grad
                 node._grad = None
     
     def zero_(self):
         self._grad = np.zeros_like(self.data)
     
     def retain_grad(self):
```

## synapgrad/nn/__init__.py

```diff
@@ -1,6 +1,6 @@
 
 from .modules import Module, Sequential
 from .neurons import Neuron
 from .layers import Linear
-from .losses import Loss, MSELoss
-from .activations import ReLU, Sigmoid
+from .losses import Loss, MSELoss, CrossEntropyLoss
+from .activations import ReLU, Sigmoid, Softmax
```

## synapgrad/nn/activations.py

```diff
@@ -20,39 +20,32 @@
 
 class Sigmoid(nn.Module):
     
     def forward(self, x:Tensor) -> Tensor:
         # Returning (1/(1 + np.e**-x)) should be enough, but defining explicit 
         # grad function should be faster
         assert isinstance(x, Tensor), "Input must be a Tensor"
-        sigmoid = 1/(1 + np.e**-x.data)
+        sigmoid = 1/(1 + np.exp(-x.data))
         out = Tensor(sigmoid, (x,), '<Sigmoid>', requires_grad=x.requires_grad)
     
         def _backward():
             if x.requires_grad:
                 f = 1/(1 + np.exp(-x.data))
                 x_grad = f * (1 - f)
                 
-                x._grad = (x_grad) * out._grad
+                x._grad += (x_grad) * out._grad
             
         out._backward = _backward
         
         return out
     
     
-# class Softmax:
+class Softmax(nn.Module):
+    """ Reference: https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/ """
     
-#     def __init__(self) -> None:
-#         super().__init__()
-#         self.trainable = False
-    
-#     def __call__(self, x:np.ndarray) -> np.ndarray:
-#         super().__call__(x)
-#         softmax = []     
-#         for sample in x:
-#             exp = np.exp(sample)
-#             out = exp / np.sum(exp)
-#             softmax.append(out)
+    def __init__(self, dim) -> None:
+        super().__init__()
+        self.dim = dim
     
-#         self.output = np.array(softmax)
-
-#         return self.output
+    def forward(self, x: Tensor) -> Tensor:
+        assert isinstance(x, Tensor), "Input must be a Tensor"
+        return x.exp() / (x.exp().sum(-1)).unsqueeze(-1)
```

## synapgrad/nn/losses.py

```diff
@@ -1,25 +1,23 @@
 from typing import Any
 from abc import ABC, abstractmethod
 from .. import Tensor
+import numpy as np
 
 
 epsilon = 1e-7
 
 
 class Loss(ABC):
     
     def __init__(self, reduction='mean') -> None:
         self.reduction = reduction
         
     def __call__(self, y_pred:Tensor, y_true:Tensor) -> Any:
         assert y_pred.matches_shape(y_true), f"Inputs shape don't match y_pred={y_pred.shape}, y_true={y_true.shape}"
-        
-        if len(y_pred.shape) > 1:
-            raise ValueError("Module expects a batched input of Shape=(batch_size,)")
 
         loss = self.criterion(y_pred, y_true)
         
         # Reduction
         if self.reduction == 'sum':
             loss = loss.sum()
         elif self.reduction == 'mean':
@@ -34,76 +32,49 @@
 
 class MSELoss(Loss):
     """ Mean Squared Error Loss: (y_pred - y_true)**2 """
     
     def criterion(self, y_pred:Tensor, y_true:Tensor) -> Tensor:
         assert isinstance(y_pred, Tensor) and isinstance(y_true, Tensor), "Inputs must be Tensors"
         req_grad = y_pred.requires_grad or y_true.requires_grad
-        loss = Tensor((y_pred.data - y_true.data)**2, (y_pred, y_true), '<MeanSquaredError>', requires_grad=req_grad)
+        loss = Tensor((y_pred.data - y_true.data)**2, (y_pred, y_true), '<MSELoss>', requires_grad=req_grad)
         
         def _backward():
             grad = 2*(y_pred.data - y_true.data) * loss._grad
-            if y_pred.requires_grad: y_pred._grad += grad
-            if y_true.requires_grad: y_true._grad += grad
+            if y_pred.requires_grad: y_pred._grad += grad # * loss._grad
+            if y_true.requires_grad: y_true._grad += grad # * loss._grad
         
         loss._backward = _backward
         
         return loss
     
 
 # class BCELoss(Loss):
+#     # TODO: Not working properly
     
 #     def __init__(self) -> None:
 #         pass
     
 #     def __call__(self, y_pred:np.ndarray, y_true:np.ndarray) -> float:
 #         super().__call__(y_pred, y_true)
 #         y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
 #         term_0 = (1-y_true) * np.log(1-y_pred + epsilon)
 #         term_1 = y_true * np.log(y_pred + epsilon)
         
 #         return -np.mean(term_0+term_1, axis=0)
     
     
-# class CrossEntropyLoss:
-
-#     def __init__(self) -> None:
-#         pass
-    
-#     def __call__(self, y_pred:np.ndarray, y_true:np.ndarray) -> np.ndarray:
-#         super().__call__(y_pred, y_true)
-#         total_loss = []
-#         for pred, true in zip(y_pred, y_true):
-#             loss = -np.sum(true * np.log(pred + epsilon))
-#             total_loss.append(loss)
-        
-#         total_loss = np.array(total_loss)
-        
-#         return total_loss
-
-#     def backward(self):
-#         """Gradient of the cross-entropy loss function for p and y.
-#         p: (T, 1) vector of predicted probabilities.
-#         y: (T, 1) vector of expected probabilities; must be one-hot -- one and only
-#                 one element of y is 1; the rest are 0.
-#         Returns a (1, T) Jacobian for this function.
-#         """
-#         #print(self.y_pred, self.y_pred.shape, self.y_true, self.y_true.shape)
-#         loss_gradient = []
-#         for pred, true in zip(self.y_pred, self.y_true):
-#             pred = np.expand_dims(pred, axis=-1)
-#             true = np.expand_dims(true, axis=-1)
-#             assert(pred.shape == true.shape and pred.shape[1] == 1)
-#             # py is the value of p at the index where y == 1 (one and only one such
-#             # index is expected for a one-hot y).
-#             py = pred[true == 1]
-#             #print(py)
-#             assert(py.size == 1)
-#             # D is zeros everywhere except at the index where y == 1. The final D has
-#             # to be a row-vector.
-#             D = np.zeros_like(pred)
-#             D[pred == 1] = -1/py.flat[0]
-#             loss_gradient.append(D.flatten())
-        
-#         loss_gradient = np.array(loss_gradient)
-#         print(loss_gradient, loss_gradient.shape)
-#         return loss_gradient
+class CrossEntropyLoss(Loss):
+    """ Reference: https://deepnotes.io/softmax-crossentropy#:~:text=Cross%20entropy%20indicates%20the%20distance,used%20alternative%20of%20squared%20error"""
+    # TODO: Not working properly
+    
+    def criterion(self, y_pred: Tensor, y_true: Tensor) -> Tensor:
+        """
+        X is the output from fully connected layer (num_examples x num_classes)
+        y is labels (num_examples x 1)
+            Note that y is not one-hot encoded vector. 
+            It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.
+        """
+        assert isinstance(y_pred, Tensor) and isinstance(y_true, Tensor), "Inputs must be Tensors"
+        return -y_pred[range(y_true.shape[0]), y_true.data.argmax(axis=1)].log().mean()
+                    
+
```

## synapgrad/optim/optimizers.py

```diff
@@ -1,11 +1,12 @@
 
 from abc import ABC, abstractmethod
 
 from .. import engine, Tensor
+import numpy as np
 
 
 class Optimizer(ABC):
     
     def __init__(self, parameters:list[Tensor], lr=0.001) -> None:
         super().__init__()
         self.parameters = parameters
@@ -25,12 +26,41 @@
     def step(self):
         with engine.no_grad():
             for p in self.parameters:
                 p.data -= self.lr*p._grad
         
     
 class Adam(Optimizer):
+    # TODO: Nos working properly
+    
+    def __init__(self, parameters: list[Tensor], lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8) -> None:
+        """Simplified Adam optimizer -> https://pytorch.org/docs/stable/generated/torch.optim.Adam.html
+            amsgrad = False, maximize = False, weight_decay = 0
+        Args:
+            beta1 (float): Exponential decay rate for the moving average of the gradient.
+            beta2 (float): Exponential decay rate for the moving average of the squared gradient.
+            epsilon (float): Small value for numerical stability.
+        """
+        super().__init__(parameters, lr)
+        self.beta1 = beta1
+        self.beta2 = beta2
+        self.epsilon = epsilon
+        
+        self.mt = [0 for _ in range(len(parameters))]
+        self.vt = [0 for _ in range(len(parameters))]
     
     def step(self):
+        
         with engine.no_grad():
-            for p in self.parameters:
-                ...
+            for i, p in enumerate(self.parameters):
+                # Update the moving average of the gradient
+                m = self.beta1 * self.mt[i] + (1 - self.beta1) * p._grad
+                self.mt[i] = m
+                m_corrected = m / (1 - self.beta1)
+
+                # Update the moving average of the squared gradient
+                v = self.beta2 * self.vt[i]+ (1 - self.beta2) * p._grad**2
+                self.vt[i] = v
+                v_corrected = v / (1 - self.beta2)
+
+                # Update the parameters using the Adam formula
+                p.data -= self.lr * m_corrected / (np.sqrt(v_corrected) + self.epsilon)
```

## synapgrad/utils/__init__.py

```diff
@@ -0,0 +1,9 @@
+00000000: 0d0a 6672 6f6d 202e 2069 6d70 6f72 7420  ..from . import 
+00000010: 6461 7461 2c20 6772 6170 682c 2074 7261  data, graph, tra
+00000020: 696e 0d0a 6672 6f6d 202e 7472 6169 6e20  in..from .train 
+00000030: 696d 706f 7274 2054 7261 696e 6572 2c20  import Trainer, 
+00000040: 4576 616c 7561 746f 720d 0a66 726f 6d20  Evaluator..from 
+00000050: 2e64 6174 6120 696d 706f 7274 2073 706c  .data import spl
+00000060: 6974 5f64 6174 6173 6574 2c20 6f6e 655f  it_dataset, one_
+00000070: 686f 745f 656e 636f 6465 2c20 4461 7461  hot_encode, Data
+00000080: 4c6f 6164 6572                           Loader
```

## synapgrad/utils/graph.py

```diff
@@ -1,35 +1,38 @@
 
 from graphviz import Digraph
 
 
 def trace(root):
     nodes, edges = set(), set()
-    def build(v):
-        if v not in nodes:
-            nodes.add(v)
-            for child in v._prev:
-                edges.add((child, v))
+    def build(n):
+        if n not in nodes:
+            nodes.add(n)
+            for child in n._children:
+                edges.add((child, n))
                 build(child)
     build(root)
     return nodes, edges
 
 def draw_dot(root, format='svg', rankdir='LR'):
     """
     format: png | svg | ...
     rankdir: TB (top to bottom graph) | LR (left to right)
     """
     assert rankdir in ['LR', 'TB']
     nodes, edges = trace(root)
     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})
     
+    
     for n in nodes:
-        dot.node(name=str(id(n)), label = "{ data %.4f | grad %.4f }" % (n.data, n.grad), shape='record')
-        if n._op:
-            dot.node(name=str(id(n)) + n._op, label=n._op)
-            dot.edge(str(id(n)) + n._op, str(id(n)))
+        nid = str(id(n))
+        grad = None if n._grad is None else n._grad.round(decimals=2)
+        dot.node(name=nid, label=f"tensor={n.data.round(decimals=2)} | req_grad={n.requires_grad}, is_leaf={n.is_leaf} | grad={grad}", shape='record')
+        if n._operation:
+            dot.node(name=nid + n._operation, label=n._operation)
+            dot.edge(nid + n._operation, nid)
     
     for n1, n2 in edges:
-        dot.edge(str(id(n1)), str(id(n2)) + n2._op)
+        dot.edge(str(id(n1)), str(id(n2)) + n2._operation)
     
     return dot
```

## Comparing `synapgrad-0.1.1.dist-info/LICENSE` & `synapgrad-0.2.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `synapgrad-0.1.1.dist-info/METADATA` & `synapgrad-0.2.0.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: synapgrad
-Version: 0.1.1
+Version: 0.2.0
 Summary: An autograd Tensor-based engine with a deep learning library built on top of it made from scratch
 Home-page: https://github.com/pgmesa/synapgrad
 Author: Pablo GarcÃ­a Mesa
 Author-email: pgmesa.sm@gmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.9
@@ -15,32 +15,30 @@
 License-File: LICENSE
 Requires-Dist: numpy (>=1.21.0)
 
 #  SynapGrad
 
 An autograd Tensor-based engine with a deep learning library built on top of it made from scratch
 
-[ ! ] This project is currently being developed...
-
 ## Technical Description
 This project implements a completely functional engine for tracking operations between Tensors, by dynamically building a Directed Acyclic Graph (DAG), and an automatic backpropagation algorithm (reverse-mode autodiff) over this DAG.
 
 Built on top of the engine, the deep learning library implements the most common functions, layers, losses and optimizers in order to create MLPs and CNNs able to solve AI problems
 
-This library tries to mimic Pytorch in a very simplified way, but with similar functions and behaviour. 
+This library tries to mimic Pytorch in a simplified way, but with similar functions and behaviour. 
 
 ## Aim of the project
 The aim of this project is to create a deep learning library from scratch, without using any existing framework (such as keras, pytorch, tensorflow, sklearn, etc) in order to fully understand the core aspects of how they work. Specifically, this time I have focused on pytorch.
 
 Some of the external frameworks mentioned before have been used for the following reasons:
 
 - (pytorch) to check gradient calculation is correct
 - (sklearn) (keras) to download the example datasets
 
-This project is based on the amazing educational project `micrograd` from `karpathy` (https://github.com/karpathy/micrograd)
+This project stems from the amazing educational project `micrograd` by `karpathy` (https://github.com/karpathy/micrograd)
 
 Note: Supporting GPU execution is out of the scope of this project
 
 ## Installation
 ```bash
 pip install synapgrad
 ```
@@ -67,39 +65,57 @@
 
 print(a.grad) # da/dout
 print(c.grad) # dc/dout
 ```
 
 ## Training examples using synapgrad
 
-This project comes with 3 jupyter notebooks that solve 3 beginner's problems in AI:
+This project comes with 3 jupyter notebooks (in `examples/`) that solve 3 beginner's problems in AI:
 
 - [x] 1. Basic MLP for binary classification (sklearn 'make_moons' toy dataset)
 - [ ] 2. MLP for handwritten digits classification (MNIST dataset) 
 - [ ] 3. CNN for handwritten digits classification (MNIST dataset)
 
+So far, only the first example has been implemented
+
 Example 1 (synapgrad MLP solution)     |  Example 2 and 3
 :-------------------------:|:-------------------------:
 ![Board Image](/assets/example1.png) | ![Check Image](/assets/example23.png) 
 
 ## Comparisons with other frameworks
-In order to see the efficiency of synapgrad, it is compared with other existing engines (torch and micrograd).
+In order to see the efficiency of synapgrad, it is compared with other existing engines (in this case torch and micrograd).
 
 
 | Training Example | synapgrad | torch | micrograd |
 |     :---:        |  :---:  |  :---:  |   :---:   |  
 | 1  | 1.7 s | 1.5 s | 1 min y 43 s |
 | 2  | - | - | - |
 | 3  | - | - | - |
 
-As you can see, synapgrad is very efficient
+As you can see, synapgrad is fast
 
 ## Graph Visualization
+In the `examples/trace_graph.ipynb` notebook there is an example of how to display the graph that synapgrad creates in the background as operations are chained.
+
+```python
+import synapgrad
+from synapgrad import nn, utils
+
+with synapgrad.retain_grads():
+    x = synapgrad.tensor([5.0, 3.0], requires_grad=True)
+    x2 = synapgrad.tensor([6.0, 0.4], requires_grad=True)
+    y = (x ** 3 + 3) 
+    y2 = (x2 / x)
+    z = y * y2 * x2
+    z = z.sum()
+    z.backward()
+utils.graph.draw_dot(z)
 ```
-```
+
+![Board Image](/assets/graph_example.svg)
 
 ## Running tests
-To run the unit tests you will have to install PyTorch. In this tests, gradients calculation as well as losses, layers, etc, are assessed with pytorch to check everything is working fine. To run the tests:
+To run the unit tests you will have to install PyTorch. In these tests, gradients calculation as well as losses, layers, etc, are assessed with pytorch to check everything is working fine. To run the tests:
 ```bash
 python -m pytest
 ```
```

## Comparing `synapgrad-0.1.1.dist-info/RECORD` & `synapgrad-0.2.0.dist-info/RECORD`

 * *Files 26% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-synapgrad/__init__.py,sha256=YECLsV2aLYGwW5o3zqW1pO6PrB7zKjawrYjij2fcsrQ,58
-synapgrad/engine.py,sha256=-kT4wry6AyRlGw-DB9jlHrQiyua0s3JKaip3GDJQ7Qw,17246
-synapgrad/nn/__init__.py,sha256=gP5rsa7GRJyqCp483bGReFUmZh7nucMGHnNMrangyII,173
-synapgrad/nn/activations.py,sha256=tjIVZwKNHeGFzBEulbxLQLkT_WP7ZJpC5d-U6y9cdHA,1643
+synapgrad/__init__.py,sha256=rzU1koPmJ4_LyMdxWzho4Qd-qWUqh16aKSgmF5HBOEw,72
+synapgrad/engine.py,sha256=Q6V3bxGjbqy2gaKTQ_QwODxauiSPO3BxMQyxaP38bNs,19226
+synapgrad/nn/__init__.py,sha256=FQdK4EOu1AUInboEqQXF_ERITrqk7vc-skKizzOfLwk,200
+synapgrad/nn/activations.py,sha256=nRcc-k3Pl9kqT6PSdV1R26OnjFENg0fyoezzklUO36g,1577
 synapgrad/nn/layers.py,sha256=gwiskPgGLaLY66g_w1QarnHWmMTDcaIYic2kR9aqPTw,1666
-synapgrad/nn/losses.py,sha256=m4rXIdL7wXE6WyeRRx99jT_2gzqNKm9wVwPRzxOP2qU,3935
+synapgrad/nn/losses.py,sha256=0aW0mwCkXZ7IH8IySmfxKZ4YfPsH9F-3ItEBADPSzME,2867
 synapgrad/nn/modules.py,sha256=ex4V9cf2lHXN0f7lR9o0lXMlHqDr5JJnDkPncWNoZXI,2413
 synapgrad/nn/neurons.py,sha256=cmapUttse6pZcnxFPWSS3TyAK6tizNLJ0sUYzo1nkPA,2131
 synapgrad/optim/__init__.py,sha256=8X56jD7Dcyw-Hdux1K7_YK9_oc3BuNPk9YwC7gSNWaE,46
-synapgrad/optim/optimizers.py,sha256=TZXvkIj-AvVMIef0WbQ1yWpOobQESsP49E9e0ygu_fk,746
-synapgrad/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+synapgrad/optim/optimizers.py,sha256=CK0NlIFPwRyYaQu0UaYTTtk-OCx2pXvu8Yr5cRiLpKw,2226
+synapgrad/utils/__init__.py,sha256=1Qup8SVDUE3V04IuHVCe5Qdb7AlYqEVjA4BQYcnSIuU,134
 synapgrad/utils/data.py,sha256=s2Nsop5Jhkjmb9pKfRz_F0hHik8qd36NgM92ZlhMb3o,2965
-synapgrad/utils/graph.py,sha256=hI8FCASmlu2j1ONrxeYNxG8UYhhuevUJ8vTXZ_z-tH0,1007
+synapgrad/utils/graph.py,sha256=EBiVOLu_PGOK33doNcG_DoUzV-Mj-1qUntuzmWTttN8,1173
 synapgrad/utils/train.py,sha256=3ZedAmb244WI4UlaAtM2cJqYerw4ZKAMNQXUIdb_Kuw,10050
-synapgrad-0.1.1.dist-info/LICENSE,sha256=tILw81bLBHb_8uhiXZAlAo5QrlyMSCJH33lb7GYpz6E,1088
-synapgrad-0.1.1.dist-info/METADATA,sha256=Ff3I-xZTOI1i9-kY0OLICEf6yzWzMrf0CaKPY76ghGA,3895
-synapgrad-0.1.1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-synapgrad-0.1.1.dist-info/top_level.txt,sha256=D7_rcC0S5ytl1AFRAo5c73mZggZB-Z_zm_GNIGe0QgM,10
-synapgrad-0.1.1.dist-info/RECORD,,
+synapgrad-0.2.0.dist-info/LICENSE,sha256=tILw81bLBHb_8uhiXZAlAo5QrlyMSCJH33lb7GYpz6E,1088
+synapgrad-0.2.0.dist-info/METADATA,sha256=RxACwb0uIBDj_ZK960np8RmltooKZtotLjm7xkR42VY,4433
+synapgrad-0.2.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+synapgrad-0.2.0.dist-info/top_level.txt,sha256=D7_rcC0S5ytl1AFRAo5c73mZggZB-Z_zm_GNIGe0QgM,10
+synapgrad-0.2.0.dist-info/RECORD,,
```

